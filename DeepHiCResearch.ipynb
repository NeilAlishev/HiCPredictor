{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Notes\n",
    "\n",
    "в пределах 10мб - не мусорные контакты\n",
    "всю Hi-C карту предсказывать не надо. участки вдоль диагонали\n",
    "\n",
    "удалить главную диагональ? - схлопывать\n",
    "\n",
    "\n",
    "TODO: реализовать RMSE и R^2 для численной оценки качества предсказаний моделей\n",
    "Разделить на 80 training, 10 dev, 10 test\n",
    "Посмотреть качество предсказаний на test set'е\n",
    "\n",
    "\n",
    "TODO: \n",
    "Learn only on upper triangle (2 times less output values, should be faster). \n",
    "Then, after prediction, form 2d matrix from upper triangle\n",
    "\n",
    "TODO:\n",
    "Use all chromosomes (not only one) for training and testing NN\n",
    "\n",
    "\n",
    "TODO:\n",
    "To reduce overfitting and increase generalization\n",
    "stochastically shift input sequences by up to +/- 11 bp and reverse complement the DNA and flip the Hi-C map.\n",
    "(described in Fudenberg)\n",
    "\n",
    "\n",
    "Refactoring is needed!\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "pandas.set_option('display.max_columns', 500)\n",
    "pandas.set_option('display.max_rows', 500)\n",
    "\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import cooler\n",
    "import cooltools as ct\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "import pickle\n",
    "\n",
    "import scipy\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following directive activates inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# allow to allocate resources for model training\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.backend import set_session\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# Training set formation\n",
    "WINDOW_SIZE = 526\n",
    "STRIDE = 526 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTIL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hic(matrix, use_log_scale = False, chromosome_position = ()):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    if use_log_scale:\n",
    "        im = ax.matshow(np.log10(matrix), cmap='YlOrRd')\n",
    "        fig.colorbar(im)\n",
    "    else:\n",
    "        im = ax.matshow(matrix, cmap='YlOrRd')\n",
    "        fig.colorbar(im)\n",
    "    \n",
    "    if len(chromosome_position) != 0:\n",
    "        ax.set_title(f\"{chromosome_position[0]}: {chromosome_position[1][0]}-{chromosome_position[1][1]}\", fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_dna(sequences = {}):\n",
    "    for k,v in sequences.items():\n",
    "        seq_array = np.array(list(v))\n",
    "\n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded_seq = label_encoder.fit_transform(seq_array)\n",
    "\n",
    "        integer_encoded_seq = integer_encoded_seq.reshape(len(integer_encoded_seq), 1)\n",
    "\n",
    "        onehot_encoder = OneHotEncoder(sparse = False)\n",
    "        result = onehot_encoder.fit_transform(integer_encoded_seq)\n",
    "        \n",
    "        # if Ns are present in the DNA sequence, result will have 5 columns. We delete 4th column which has Ns\n",
    "        # N row in the resulting training set will have all 0s\n",
    "        if result.shape[1] == 5:\n",
    "            result = np.delete(result, 3, 1)\n",
    "        \n",
    "        sequences[k] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_training_squares(hic_library, chroms):\n",
    "    training_set = {}\n",
    "\n",
    "    COLAB_OFFSET = 526 # offset from the transformation step from the Google Colab\n",
    "    for chrom in chroms:\n",
    "        current_chrom = hic_library[chrom]\n",
    "        training_set[chrom] = []\n",
    "        \n",
    "        for part_number in sorted(current_chrom.keys()):\n",
    "            current_chrom_part = current_chrom[part_number]\n",
    "            \n",
    "            offset = (part_number - 1) * COLAB_OFFSET\n",
    "            \n",
    "            for i in range(WINDOW_SIZE, current_chrom_part.shape[0] + 1, STRIDE):\n",
    "                training_set[chrom].append((current_chrom_part[i - WINDOW_SIZE:i, i - WINDOW_SIZE:i],\n",
    "                                          (offset + (i - WINDOW_SIZE), offset + i)))\n",
    "            \n",
    "    return training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_train_x(sequences_one_hot, chroms, training_squares, train_indices):\n",
    "    train_x = []\n",
    "    count = 0\n",
    "    \n",
    "    CROPPING_FACTOR_DNA = 856\n",
    "    \n",
    "    for chrom in chroms:\n",
    "        cur_seq = sequences_one_hot[chrom]\n",
    "        cur_training_squares = training_squares[chrom]\n",
    "\n",
    "        for training_square in cur_training_squares:\n",
    "            sq_begin, sq_end = training_square[1]\n",
    "            \n",
    "            if count in train_indices:\n",
    "                # crop train_x to match 524288\n",
    "                train_x.append(cur_seq[(sq_begin * 1000) + CROPPING_FACTOR_DNA:(sq_end * 1000) - CROPPING_FACTOR_DNA, :])\n",
    "\n",
    "            count += 1\n",
    "            \n",
    "    return np.asarray(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_train_y(training_squares, chroms, train_indices):\n",
    "    train_y = []\n",
    "    count = 0\n",
    "    \n",
    "    for chrom in chroms:            \n",
    "        cur_training_squares = training_squares[chrom]\n",
    "\n",
    "        for training_square in cur_training_squares:\n",
    "            CROPPING_TARGET = 7\n",
    "            cropped_training_square = training_square[0][CROPPING_TARGET:training_square[0].shape[0] - CROPPING_TARGET, \n",
    "                                                         CROPPING_TARGET:training_square[0].shape[1] - CROPPING_TARGET]\n",
    "\n",
    "            upper_triu = to_upper_triu(cropped_training_square, diagonal_offset=2)\n",
    "            \n",
    "            if count in train_indices:\n",
    "                train_y.append(upper_triu)\n",
    "                \n",
    "            count += 1   \n",
    "            \n",
    "    return np.asarray(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_upper_triu(input_matrix, diagonal_offset = 2):\n",
    "    seq_len = input_matrix.shape[0]\n",
    "    return input_matrix[np.triu_indices(seq_len, diagonal_offset)].reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab specific code\n",
    "# filepath = \"drive/My Drive/Colab Notebooks/S2-Wang2017-async.dm3.mapq_30.100.mcool\"\n",
    "\n",
    "filepath = \"S2-Wang2017-async.dm3.mapq_30.100.mcool\"\n",
    "\n",
    "resolution = \"::/resolutions/1000\" # 1 KB resolution\n",
    "c = cooler.Cooler(filepath + resolution)\n",
    "\n",
    "chroms = c.chromnames\n",
    "# don't use these small chromosomes\n",
    "chroms.remove(\"chrM\")\n",
    "chroms.remove(\"chr4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T RUN, LOAD TRANSFORMED Hi-C INSTEAD\n",
    "for chrom in chroms:\n",
    "    arr = c.matrix(balance=True).fetch(chrom)\n",
    "    # For adaptive coarse-grain transformation purposes\n",
    "    arr_raw = c.matrix(balance=False).fetch(chrom)\n",
    "    \n",
    "    transformed_arr = transform_hic(arr, arr_raw)\n",
    "    file_name = chrom + \"_transformed.npy\"\n",
    "    np.save(f\"./transformed_hic/{file_name}\", transformed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(arr, use_log_scale = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all transformed Hi-C\n",
    "transformed_hic = {}\n",
    "for chrom in chroms:\n",
    "    chrom_hic_filenames = [filename for filename in os.listdir('./transformed_hic') if filename.startswith(chrom)]\n",
    "    \n",
    "    if len(chrom_hic_filenames) > 0:\n",
    "        chrom_parts = {}\n",
    "        for filename in chrom_hic_filenames:\n",
    "            current_part_number = int(re.search(\"(transformed)(\\d+)\", filename).group(2))\n",
    "            chrom_parts[current_part_number] = np.load(f\"./transformed_hic/{filename}\")['arr_0']\n",
    "\n",
    "        transformed_hic[chrom] = chrom_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_squares = select_training_squares(transformed_hic, chroms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = {}\n",
    "for chrom in chroms:\n",
    "    fasta_sequence = list(SeqIO.parse(open(f\"./chromFa/{chrom}.fa\"),'fasta'))[0]\n",
    "    sequences[chrom] = str(fasta_sequence.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Воспринимаю маленькие и большие буквы одинаковым образом\n",
    "for k in sequences.keys():\n",
    "    sequences[k] = sequences[k].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For later use (augmentation)\n",
    "sequences_orig = sequences.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразую последовательности в one-hot encoding (A = 0, C = 1, G = 2, T = 3)\n",
    "one_hot_dna(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 60/40 - train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_indices = np.random.choice(1836, 1150, replace = False) \n",
    "# 1150 - is 60 percent of the whole dataset\n",
    "# np.save(\"./train_indices.npy\", train_indices)\n",
    "\n",
    "# LOAD FROM THE MEMORY\n",
    "train_indices = np.load(\"./train_test_data/train_indices.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.load(\"./train_test_data/train_indices.npy\")\n",
    "test_indices = np.load(\"./train_test_data/test_indices.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = form_train_x(sequences, chroms, training_squares, test_indices)\n",
    "test_y = form_train_y(training_squares, chroms, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"./test_x.npz\", test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"./test_y.npz\", test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = form_train_x(sequences, chroms, training_squares, train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"./train_x.npz\", train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = form_train_y(training_squares, chroms, train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"./train_y.npz\", train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convnet model (similar to Fudenberg NN):\n",
    "\n",
    "model_m = Sequential()\n",
    "\n",
    "model_m.add(layers.Conv1D(25, 50, activation='relu', input_shape=(50000, 4)))\n",
    "model_m.add(layers.Conv1D(25, 50, activation='relu'))\n",
    "model_m.add(layers.MaxPooling1D(5, strides = 2))\n",
    "\n",
    "model_m.add(layers.Conv1D(50, 25, activation='relu'))\n",
    "model_m.add(layers.MaxPooling1D(5, strides = 2))\n",
    "\n",
    "model_m.add(layers.Conv1D(50, 25, activation='relu'))\n",
    "model_m.add(layers.MaxPooling1D(20, strides = 4))\n",
    "\n",
    "model_m.add(layers.Conv1D(70, 20, activation='relu'))\n",
    "model_m.add(layers.MaxPooling1D(25, strides = 4))\n",
    "\n",
    "# dilated layers\n",
    "model_m.add(layers.Conv1D(100, 15, activation='relu', dilation_rate = 2))\n",
    "model_m.add(layers.Conv1D(100, 15, activation='relu', dilation_rate = 2))\n",
    "model_m.add(layers.MaxPooling1D(25, strides = 4))\n",
    "\n",
    "model_m.add(layers.Flatten())\n",
    "model_m.add(layers.Dense(2500, activation='linear'))\n",
    "\n",
    "# здесь можно не использовать входы (50 штук) и схлопывать после предсказания НС\n",
    "# не 2.500, а 2.450\n",
    "# для тренировки и для предсказаний надо будет сначала делать трансформацию для вектора\n",
    "\n",
    "print(model_m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
    "              loss='mse',\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_m.fit(train_x,\n",
    "                      train_y,\n",
    "                      batch_size=32,\n",
    "                      epochs=3,\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.load_weights('project2_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(training_squares[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_begin, sq_end = training_squares[0][1]\n",
    "seq_to_use = current_seq_one_hot[(sq_begin * 1000):(sq_end * 1000), ].reshape((1,50000, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model_m.predict(seq_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(prediction.reshape((50, 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.load_weights('project2_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(training_squares[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_begin, sq_end = training_squares[3][1]\n",
    "seq_to_use = current_seq_one_hot[(sq_begin * 1000):(sq_end * 1000), ].reshape((1,50000, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model_m.predict(seq_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(prediction.reshape((50, 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fudenberg избавляется от scaling'а\n",
    "# избавиться от шкалирования. полимерное свойство хроматина нам не обязательно выучивать\n",
    "# Нам надо это делать:\n",
    "# observed/expected - Саша отправит\n",
    "# можно применять observed/expected для всей хромосомы или для каждого квардрата 50x50\n",
    "# для каждого квардрата делать observed/expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Басет работа - 2015 (размеры слоев и параметры оттуда)\n",
    "# В басет предсказывается DNA sequence -> accessibility (open versus closed chromatin) of this area \n",
    "# (in different cell types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convnet model (similar to Basset NN):\n",
    "# Basset NN is not trainable in a reasonable time with this 50x50 window size -> reduced to 25x25 window size\n",
    "# Nevertheless, 3 times more tunable parameters than Fudenberg\n",
    "\n",
    "model_m = Sequential()\n",
    "\n",
    "model_m.add(layers.Conv1D(300, 21, activation='relu', input_shape=(25000, 4)))\n",
    "model_m.add(layers.MaxPooling1D(4))\n",
    "\n",
    "model_m.add(layers.Conv1D(300, 6, activation='relu'))\n",
    "model_m.add(layers.MaxPooling1D(4))\n",
    "\n",
    "model_m.add(layers.Conv1D(500, 4, activation='relu'))\n",
    "model_m.add(layers.MaxPooling1D(4))\n",
    "\n",
    "model_m.add(layers.Flatten())\n",
    "\n",
    "model_m.add(layers.Dense(1000, activation='relu'))\n",
    "\n",
    "model_m.add(layers.Dense(625, activation='linear'))\n",
    "\n",
    "print(model_m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.load_weights('project2_model_Basset.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(training_squares[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_begin, sq_end = training_squares[3][1]\n",
    "seq_to_use = current_seq_one_hot[(sq_begin * 1000):(sq_end * 1000), ].reshape((1,25000, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model_m.predict(seq_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(prediction.reshape((25, 25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hi-C tranformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(arr[0:1000, 0:1000], use_log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Adaptive coarse-grain\n",
    "transformed_arr = ct.lib.numutils.adaptive_coarsegrain(arr[0:1000, 0:1000], arr_raw[0:1000, 0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr, use_log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Normalize the contact matrix for distance-dependent contact decay.\n",
    "# Observed/Expected\n",
    "transformed_arr, _, _, _ = ct.lib.numutils.observed_over_expected(transformed_arr, mask = ~np.isnan(transformed_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr, use_log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Take natural logarithm\n",
    "transformed_arr = np.log(transformed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Interpolate all NaN values\n",
    "transformed_arr = ct.lib.numutils.interp_nan(transformed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Use Gaussian filter\n",
    "# To get rid of noise, emphasizing larger patterns.\n",
    "transformed_arr = scipy.ndimage.gaussian_filter(transformed_arr, sigma = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the transformations in one function\n",
    "def transform_hic(hic_matrix, hic_matrix_raw):\n",
    "    transformed_arr = ct.lib.numutils.adaptive_coarsegrain(hic_matrix, hic_matrix_raw)\n",
    "    transformed_arr, _, _, _ = ct.lib.numutils.observed_over_expected(transformed_arr, mask = ~np.isnan(transformed_arr))\n",
    "    transformed_arr = np.log(transformed_arr)\n",
    "    transformed_arr = ct.lib.numutils.interp_nan(transformed_arr)\n",
    "    transformed_arr = scipy.ndimage.gaussian_filter(transformed_arr, sigma = 1)\n",
    "    \n",
    "    return transformed_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(arr[0:4000, 0:4000], use_log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transform_hic(arr[2500:3000, 2500:3000], arr_raw[2500:3000, 2500:3000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: RMSE and R^2. split on train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (None, 5, 3) -> (None, 5, 5, 3)\n",
    "# Explanation of the 1D -> 2D procedure\n",
    "\n",
    "oned = tf.constant([[[1, 6, 11], \n",
    "                     [2, 7, 12], \n",
    "                     [3, 8, 13],\n",
    "                     [4, 9, 14],\n",
    "                     [5, 10, 15]],\n",
    "                   \n",
    "                    [[1, 6, 11], \n",
    "                     [2, 7, 12], \n",
    "                     [3, 8, 13],\n",
    "                     [4, 9, 14],\n",
    "                     [5, 10, 15]],\n",
    "                   \n",
    "                    [[1, 6, 11], \n",
    "                     [2, 7, 12], \n",
    "                     [3, 8, 13],\n",
    "                     [4, 9, 14],\n",
    "                     [5, 10, 15]]])\n",
    "\n",
    "_, seq_len, features = oned.shape\n",
    "twod1 = tf.tile(oned, [1, seq_len, 1])\n",
    "twod1 = tf.reshape(twod1, [-1, seq_len, seq_len, features])\n",
    "twod2 = tf.transpose(twod1, [0,2,1,3])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Original tiled tensor for one 1D filter (1D filter expanded 5 times)\")\n",
    "    print(np.array(sess.run([twod1]))[0][0][:, :, 0])\n",
    "    \n",
    "    print(\"Transposed tiled tensor\")\n",
    "    print(np.array(sess.run([twod2]))[0][0][:, :, 0])\n",
    "\n",
    "\n",
    "twod1 = tf.expand_dims(twod1, axis=-1)\n",
    "twod2 = tf.expand_dims(twod2, axis=-1)\n",
    "twod  = tf.concat([twod1, twod2], axis=-1)\n",
    "twod = tf.reduce_mean(twod, axis=-1)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Result of mean operation between original and transposed (mean between each pair of values in 1D filter)\")\n",
    "    print(np.array(sess.run([twod]))[0][0][:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exlanation of ConcatDist2D procedure\n",
    "# This layer adds one more channel which contains pairwise distances for the matrices obtained on the previous layer\n",
    "# (this should enhance model performance)\n",
    "# (None, 5, 5, 3) -> (None, 5, 5, 4)\n",
    "\n",
    "# For one training example -> (1,5,5,3)\n",
    "inputs = tf.random.uniform(shape=[1,5,5,3])\n",
    "\n",
    "input_shape = tf.shape(inputs)\n",
    "batch_size, seq_len = input_shape[0], input_shape[1]\n",
    "\n",
    "## concat 2D distance ##\n",
    "pos = tf.expand_dims(tf.range(0, seq_len), axis=-1)\n",
    "matrix_repr1 = tf.tile(pos, [1,seq_len])\n",
    "matrix_repr2 = tf.transpose(matrix_repr1, [1,0])\n",
    "dist  = tf.math.abs( tf.math.subtract(matrix_repr1, matrix_repr2) )\n",
    "dist = tf.dtypes.cast(dist, tf.float32)\n",
    "dist = tf.expand_dims(dist, axis=-1)\n",
    "dist = tf.expand_dims(dist, axis=0)\n",
    "dist = tf.tile(dist, [batch_size, 1, 1, 1])\n",
    "\n",
    "res = tf.concat([inputs, dist], axis=-1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(np.array(sess.run([res]))[0, :, :, :, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Custom Keras layers\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneToTwo(tf.keras.layers.Layer):\n",
    "    ''' Transform 1d to 2d with i,j vectors operated on.'''\n",
    "    def __init__(self, operation='mean'):\n",
    "        super(OneToTwo, self).__init__()\n",
    "        self.operation = operation.lower()\n",
    "\n",
    "    def call(self, oned):\n",
    "        _, seq_len, features = oned.shape\n",
    "\n",
    "        twod1 = tf.tile(oned, [1, seq_len, 1])\n",
    "        twod1 = tf.reshape(twod1, [-1, seq_len, seq_len, features])\n",
    "        twod2 = tf.transpose(twod1, [0,2,1,3])\n",
    "\n",
    "        twod1 = tf.expand_dims(twod1, axis=-1)\n",
    "        twod2 = tf.expand_dims(twod2, axis=-1)\n",
    "        twod  = tf.concat([twod1, twod2], axis=-1)\n",
    "        twod = tf.reduce_mean(twod, axis=-1)\n",
    "\n",
    "        return twod\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config['operation'] = self.operation\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDist2D(tf.keras.layers.Layer):\n",
    "    ''' Concatenate the pairwise distance to 2d feature matrix.'''\n",
    "    def __init__(self):\n",
    "        super(ConcatDist2D, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, seq_len = input_shape[0], input_shape[1]\n",
    "\n",
    "        ## concat 2D distance ##\n",
    "        pos = tf.expand_dims(tf.range(0, seq_len), axis=-1)\n",
    "        matrix_repr1 = tf.tile(pos, [1, seq_len])\n",
    "        matrix_repr2 = tf.transpose(matrix_repr1, [1, 0])\n",
    "        dist = tf.math.abs(tf.math.subtract(matrix_repr1, matrix_repr2))\n",
    "        dist = tf.dtypes.cast(dist, tf.float32)\n",
    "        dist = tf.expand_dims(dist, axis=-1)\n",
    "        dist = tf.expand_dims(dist, axis=0)\n",
    "        dist = tf.tile(dist, [batch_size, 1, 1, 1])\n",
    "        return tf.concat([inputs, dist], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Symmetrize2D(tf.keras.layers.Layer):\n",
    "    '''Take the average of a matrix and its transpose to enforce symmetry.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Symmetrize2D, self).__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        x_t = tf.transpose(x, [0, 2, 1, 3])\n",
    "        x_sym = (x + x_t) / 2\n",
    "        return x_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpperTri(tf.keras.layers.Layer):\n",
    "    ''' Unroll matrix to its upper triangular portion.'''\n",
    "\n",
    "    def __init__(self, diagonal_offset=2):\n",
    "        super(UpperTri, self).__init__()\n",
    "        self.diagonal_offset = diagonal_offset\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_len = inputs.shape[1].value\n",
    "        output_dim = inputs.shape[-1]\n",
    "\n",
    "        triu_tup = np.triu_indices(seq_len, self.diagonal_offset)\n",
    "        triu_index = list(triu_tup[0] + seq_len * triu_tup[1])\n",
    "        unroll_repr = tf.reshape(inputs, [-1, seq_len ** 2, output_dim])\n",
    "        return tf.gather(unroll_repr, triu_index, axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config['diagonal_offset'] = self.diagonal_offset\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticReverseComplement(tf.keras.layers.Layer):\n",
    "    \"\"\"Stochastically reverse complement a one hot encoded DNA sequence.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(StochasticReverseComplement, self).__init__()\n",
    "\n",
    "    def call(self, seq_1hot, training=None):\n",
    "        if training:\n",
    "            rc_seq_1hot = tf.gather(seq_1hot, [3, 2, 1, 0], axis=-1)\n",
    "            rc_seq_1hot = tf.reverse(rc_seq_1hot, axis=[1])\n",
    "            reverse_bool = tf.random.uniform(shape=[]) > 0.5\n",
    "            src_seq_1hot = tf.cond(reverse_bool, lambda: rc_seq_1hot, lambda: seq_1hot)\n",
    "            return src_seq_1hot, reverse_bool\n",
    "        else:\n",
    "            return seq_1hot, tf.constant(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticShift(tf.keras.layers.Layer):\n",
    "    \"\"\"Stochastically shift a one hot encoded DNA sequence.\"\"\"\n",
    "\n",
    "    def __init__(self, shift_max=0, pad='uniform'):\n",
    "        super(StochasticShift, self).__init__()\n",
    "        self.shift_max = shift_max\n",
    "        self.augment_shifts = tf.range(-self.shift_max, self.shift_max + 1)\n",
    "        self.pad = pad\n",
    "\n",
    "    def call(self, seq_1hot, training=None):\n",
    "        if training:\n",
    "            shift_i = tf.random.uniform(shape=[], minval=0, dtype=tf.int64,\n",
    "                                        maxval=len(self.augment_shifts))\n",
    "            shift = tf.gather(self.augment_shifts, shift_i)\n",
    "            sseq_1hot = tf.cond(tf.not_equal(shift, 0),\n",
    "                                lambda: shift_sequence(seq_1hot, shift),\n",
    "                                lambda: seq_1hot)\n",
    "            return sseq_1hot\n",
    "        else:\n",
    "            return seq_1hot\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'shift_max': self.shift_max,\n",
    "            'pad': self.pad\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "def shift_sequence(seq, shift, pad_value=0.25):\n",
    "    \"\"\"Shift a sequence left or right by shift_amount.\n",
    "    Args:\n",
    "    seq: [batch_size, seq_length, seq_depth] sequence\n",
    "    shift: signed shift value (tf.int32 or int)\n",
    "    pad_value: value to fill the padding (primitive or scalar tf.Tensor)\n",
    "    \"\"\"\n",
    "    if seq.shape.ndims != 3:\n",
    "        raise ValueError('input sequence should be rank 3')\n",
    "    input_shape = seq.shape\n",
    "\n",
    "    pad = pad_value * tf.ones_like(seq[:, 0:tf.abs(shift), :])\n",
    "\n",
    "    def _shift_right(_seq):\n",
    "        # shift is positive\n",
    "        sliced_seq = _seq[:, :-shift:, :]\n",
    "        return tf.concat([pad, sliced_seq], axis=1)\n",
    "\n",
    "    def _shift_left(_seq):\n",
    "        # shift is negative\n",
    "        sliced_seq = _seq[:, -shift:, :]\n",
    "        return tf.concat([sliced_seq, pad], axis=1)\n",
    "\n",
    "    sseq = tf.cond(tf.greater(shift, 0),\n",
    "                   lambda: _shift_right(seq),\n",
    "                   lambda: _shift_left(seq))\n",
    "    sseq.set_shape(input_shape)\n",
    "\n",
    "    return sseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchReverse(tf.keras.layers.Layer):\n",
    "    \"\"\"Reverse predictions if the inputs were reverse complemented.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SwitchReverse, self).__init__()\n",
    "\n",
    "    def call(self, x_reverse):\n",
    "        x = x_reverse[0]\n",
    "        reverse = x_reverse[1]\n",
    "\n",
    "        xd = len(x.shape)\n",
    "        if xd == 3:\n",
    "            rev_axes = [1]\n",
    "        elif xd == 4:\n",
    "            rev_axes = [1, 2]\n",
    "        else:\n",
    "            raise ValueError('Cannot recognize SwitchReverse input dimensions %d.' % xd)\n",
    "\n",
    "        return tf.keras.backend.switch(reverse,\n",
    "                                       tf.reverse(x, axis=rev_axes),\n",
    "                                       x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Helper functions\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(current, activation, verbose=False):\n",
    "    if verbose: \n",
    "        print('activate:',activation)\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        current = tf.keras.layers.ReLU()(current)\n",
    "    elif activation == 'gelu':\n",
    "        current = GELU()(current)\n",
    "    elif activation == 'sigmoid':\n",
    "        current = tf.keras.layers.Activation('sigmoid')(current)\n",
    "    elif activation == 'tanh':\n",
    "        current = tf.keras.layers.Activation('tanh')(current)\n",
    "    elif activation == 'exp':\n",
    "        current = Exp()(current)\n",
    "    elif activation == 'softplus':\n",
    "        current = Softplus()(current)\n",
    "    else:\n",
    "        print('Unrecognized activation \"%s\"' % activation, file=sys.stderr)\n",
    "        exit(1)\n",
    "\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Keras blocks\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inputs, filters=None, kernel_size=1, activation='relu', strides=1,\n",
    "    dilation_rate=1, l2_scale=0, dropout=0, conv_type='standard', residual=False,\n",
    "    pool_size=1, batch_norm=False, bn_momentum=0.99, bn_gamma=None,\n",
    "    kernel_initializer='he_normal'):\n",
    "  \n",
    "    \"\"\"Construct a single convolution block.\n",
    "    Args:\n",
    "    inputs:        [batch_size, seq_length, features] input sequence\n",
    "    filters:       Conv1D filters\n",
    "    kernel_size:   Conv1D kernel_size\n",
    "    activation:    relu/gelu/etc\n",
    "    strides:       Conv1D strides\n",
    "    dilation_rate: Conv1D dilation rate\n",
    "    l2_scale:      L2 regularization weight.\n",
    "    dropout:       Dropout rate probability\n",
    "    conv_type:     Conv1D layer type\n",
    "    residual:      Residual connection boolean\n",
    "    pool_size:     Max pool width\n",
    "    batch_norm:    Apply batch normalization\n",
    "    bn_momentum:   BatchNorm momentum\n",
    "    bn_gamma:      BatchNorm gamma (defaults according to residual)\n",
    "    Returns:\n",
    "    [batch_size, seq_length, features] output sequence\n",
    "    \"\"\"\n",
    "\n",
    "    # flow through variable current\n",
    "    current = inputs\n",
    "\n",
    "    # choose convolution type\n",
    "    if conv_type == 'separable':\n",
    "        conv_layer = tf.keras.layers.SeparableConv1D\n",
    "    else:\n",
    "        conv_layer = tf.keras.layers.Conv1D\n",
    "\n",
    "    if filters is None:\n",
    "        filters = inputs.shape[-1]\n",
    "\n",
    "    # activation\n",
    "    current = activate(current, activation)\n",
    "\n",
    "    # convolution\n",
    "    current = conv_layer(\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        use_bias=False,\n",
    "        dilation_rate=dilation_rate,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(l2_scale))(current)\n",
    "\n",
    "    # batch norm\n",
    "    if batch_norm:\n",
    "        if bn_gamma is None:\n",
    "            bn_gamma = 'zeros' if residual else 'ones'\n",
    "        \n",
    "        current = tf.keras.layers.BatchNormalization(momentum=bn_momentum, gamma_initializer=bn_gamma, fused=True)(current)\n",
    "\n",
    "    # dropout\n",
    "    if dropout > 0:\n",
    "        current = tf.keras.layers.Dropout(rate=dropout)(current)\n",
    "\n",
    "    # residual add\n",
    "    if residual:\n",
    "        current = tf.keras.layers.Add()([inputs,current])\n",
    "\n",
    "    # Pool\n",
    "    if pool_size > 1:\n",
    "        current = tf.keras.layers.MaxPool1D(pool_size=pool_size, padding='same')(current)\n",
    "\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_tower(inputs, filters_init, filters_mult=1, repeat=1, **kwargs):\n",
    "    \"\"\"Construct a reducing convolution block.\n",
    "    Args:\n",
    "    inputs:        [batch_size, seq_length, features] input sequence\n",
    "    filters_init:  Initial Conv1D filters\n",
    "    filters_mult:  Multiplier for Conv1D filters\n",
    "    repeat:        Conv block repetitions\n",
    "    Returns:\n",
    "    [batch_size, seq_length, features] output sequence\n",
    "    \"\"\"\n",
    "\n",
    "    # flow through variable current\n",
    "    current = inputs\n",
    "\n",
    "    # initialize filters\n",
    "    rep_filters = filters_init\n",
    "\n",
    "    for ri in range(repeat):\n",
    "        # convolution\n",
    "        current = conv_block(current, filters=int(np.round(rep_filters)), **kwargs)\n",
    "\n",
    "    # update filters\n",
    "    rep_filters *= filters_mult\n",
    "\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilated_residual(inputs, filters, kernel_size=3, rate_mult=2, conv_type='standard', \n",
    "                     dropout=0, repeat=1, round=False, **kwargs):\n",
    "    \"\"\"Construct a residual dilated convolution block.\n",
    "    Args:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "\n",
    "    # flow through variable current\n",
    "    current = inputs\n",
    "\n",
    "    # initialize dilation rate\n",
    "    dilation_rate = 1.0\n",
    "\n",
    "    for ri in range(repeat):\n",
    "        # For skip connection purpose\n",
    "        rep_input = current\n",
    "\n",
    "        # dilate\n",
    "        current = conv_block(current, filters=filters, kernel_size=kernel_size, \n",
    "                             dilation_rate=int(np.round(dilation_rate)), \n",
    "                             conv_type=conv_type, bn_gamma='ones', **kwargs)\n",
    "\n",
    "        # return\n",
    "        current = conv_block(current, filters=int(rep_input.shape[-1]), dropout=dropout, bn_gamma='zeros', **kwargs)\n",
    "\n",
    "        # residual add\n",
    "        current = tf.keras.layers.Add()([rep_input, current])\n",
    "\n",
    "        # update dilation rate\n",
    "        dilation_rate *= rate_mult\n",
    "        \n",
    "        if round:\n",
    "            dilation_rate = np.round(dilation_rate)\n",
    "\n",
    "    return current\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D related blocks\n",
    "\n",
    "def concat_dist_2d(inputs, **kwargs):\n",
    "    current = ConcatDist2D()(inputs)\n",
    "    return current\n",
    "\n",
    "def one_to_two(inputs, operation='mean', **kwargs):\n",
    "    current = OneToTwo(operation)(inputs)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block_2d(inputs, filters=128, activation='relu', conv_type='standard',\n",
    "    kernel_size=1, strides=1, dilation_rate=1, l2_scale=0, dropout=0, pool_size=1,\n",
    "    batch_norm=False, bn_momentum=0.99, bn_gamma='ones', symmetric=False):\n",
    "\n",
    "    \"\"\"Construct a single 2D convolution block.   \"\"\"\n",
    "\n",
    "    # flow through variable current\n",
    "    current = inputs\n",
    "\n",
    "    # activation\n",
    "    current = activate(current, activation)\n",
    "\n",
    "  # choose convolution type\n",
    "    if conv_type == 'separable':\n",
    "        conv_layer = tf.keras.layers.SeparableConv2D\n",
    "    else:\n",
    "        conv_layer = tf.keras.layers.Conv2D\n",
    "\n",
    "    # convolution\n",
    "    current = conv_layer(\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        use_bias=False,\n",
    "        dilation_rate=dilation_rate,\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(l2_scale))(current)\n",
    "\n",
    "    # batch norm\n",
    "    if batch_norm:\n",
    "        current = tf.keras.layers.BatchNormalization(\n",
    "          momentum=bn_momentum,\n",
    "          gamma_initializer=bn_gamma,\n",
    "          fused=True)(current)\n",
    "\n",
    "    # dropout\n",
    "    if dropout > 0:\n",
    "        current = tf.keras.layers.Dropout(rate=dropout)(current)\n",
    "\n",
    "    # pool\n",
    "    if pool_size > 1:\n",
    "        current = tf.keras.layers.MaxPool2D(\n",
    "          pool_size=pool_size,\n",
    "          padding='same')(current)\n",
    "\n",
    "    # symmetric\n",
    "    if symmetric:\n",
    "        current = layers.Symmetrize2D()(current)\n",
    "\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetrize_2d(inputs, **kwargs):\n",
    "    return Symmetrize2D()(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilated_residual_2d(inputs, filters, kernel_size=3, rate_mult=2,\n",
    "                        dropout=0, repeat=1, symmetric=True, **kwargs):\n",
    "    \"\"\"Construct a residual dilated convolution block.\n",
    "    \"\"\"\n",
    "\n",
    "    # flow through variable current\n",
    "    current = inputs\n",
    "\n",
    "    # initialize dilation rate\n",
    "    dilation_rate = 1.0\n",
    "\n",
    "    for ri in range(repeat):\n",
    "        rep_input = current\n",
    "\n",
    "        # dilate\n",
    "        current = conv_block_2d(current,\n",
    "                                filters=filters,\n",
    "                                kernel_size=kernel_size,\n",
    "                                dilation_rate=int(np.round(dilation_rate)),\n",
    "                                bn_gamma='ones',\n",
    "                                **kwargs)\n",
    "\n",
    "        # return\n",
    "        current = conv_block_2d(current,\n",
    "                                filters=int(rep_input.shape[-1]),\n",
    "                                dropout=dropout,\n",
    "                                bn_gamma='zeros',\n",
    "                                **kwargs)\n",
    "\n",
    "        # residual add\n",
    "        current = tf.keras.layers.Add()([rep_input, current])\n",
    "\n",
    "        # enforce symmetry\n",
    "        if symmetric:\n",
    "            current = Symmetrize2D()(current)\n",
    "\n",
    "        # update dilation rate\n",
    "        dilation_rate *= rate_mult\n",
    "\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropping_2d(inputs, cropping, **kwargs):\n",
    "    current = tf.keras.layers.Cropping2D(cropping)(inputs)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_tri(inputs, diagonal_offset=2, **kwargs):\n",
    "    current = UpperTri(diagonal_offset)(inputs)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(inputs, units, activation='softplus', kernel_initializer='he_normal',\n",
    "          l2_scale=0, l1_scale=0, **kwargs):\n",
    "    \n",
    "    current = tf.keras.layers.Dense(\n",
    "        units=units,\n",
    "        activation=activation,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        kernel_regularizer=tf.keras.regularizers.l1_l2(l1_scale, l2_scale)\n",
    "    )(inputs)\n",
    "    \n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "\n",
    "# Changed for SEQ_LENGTH = 512000\n",
    "\n",
    "#SEQ_LENGTH = 512000\n",
    "SEQ_LENGTH = 524288\n",
    "sequence = tf.keras.Input(shape=(SEQ_LENGTH, 4), name='sequence')\n",
    "\n",
    "current = sequence\n",
    "\n",
    "# Augmentation - Enable it later (after good performance on the training set)\n",
    "# current, reverse_bool = StochasticReverseComplement()(current)\n",
    "# augment_shift = 11\n",
    "# current = StochasticShift(augment_shift)(current)\n",
    "\n",
    "# TRUNK:\n",
    "\n",
    "# First 1D convolution\n",
    "current = conv_block(current, filters=96, kernel_size=11, pool_size=2, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "\n",
    "# Change for repeat = 9\n",
    "\n",
    "# Multiple (11) 1D convolutions in a tower to arrive to 2048bp representation in 1D vectors\n",
    "current = conv_tower(current, filters_init=96, filters_mult=1.0, kernel_size=5, pool_size=2, repeat=9, \n",
    "                     batch_norm=True, bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# Dilated residual layers\n",
    "current = dilated_residual(current, filters=48, rate_mult=1.75, repeat=8, dropout=0.4, batch_norm=True, \n",
    "                           bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# Bottleneck 1D convolution\n",
    "current = conv_block(current, filters=64, kernel_size=5, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "# final activation\n",
    "current = activate(current, \"relu\")\n",
    "\n",
    "\n",
    "# HEAD:\n",
    "current = one_to_two(current)\n",
    "current = concat_dist_2d(current)\n",
    "current = conv_block_2d(current, filters=48, kernel_size=3, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "current = symmetrize_2d(current)\n",
    "current = dilated_residual_2d(current, filters=24, kernel_size=3, rate_mult=1.75, repeat=6, dropout=0.1,\n",
    "                              batch_norm=True, bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# TODO: TRY WITHOUT CROP\n",
    "#current = cropping_2d(current, cropping=26)\n",
    "\n",
    "current = upper_tri(current, diagonal_offset=2)\n",
    "\n",
    "current = dense(current, units=1, activation=\"linear\")\n",
    "#current = dense(current, units=5, activation=\"linear\")\n",
    "\n",
    "# current = SwitchReverse()([current, reverse_bool])\n",
    "\n",
    "# make model\n",
    "model = tf.keras.Model(inputs=sequence, outputs=current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_old = tf.keras.Model(inputs=sequence, outputs=current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_old.load_weights(\"./weights/model_advanced_weights_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to predict on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./test_size.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = form_train_x(sequences, chroms, training_squares, test_indices)\n",
    "test_y = form_train_y(training_squares, chroms, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cooltools.lib.numutils import set_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTOR = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pred1 = sequences['chr2L'][526000 * (FACTOR - 1):526000 * FACTOR,:].reshape(1, 526000, 4)[:, 856:(526000 - 856), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromosome_position1 = ('chr2L', (526000 * (FACTOR - 1) + 856, 526000 * FACTOR - 856))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction1 = model.predict(to_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_matrix = from_upper_triu(prediction1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(prediction_matrix, chromosome_position=chromosome_position1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = c.matrix(balance=True).fetch('chr2L')[(FACTOR - 1) * 526: FACTOR * 526, (FACTOR - 1) * 526: FACTOR * 526]\n",
    "# For adaptive coarse-grain transformation purposes\n",
    "arr_raw = c.matrix(balance=False).fetch('chr2L')[(FACTOR - 1) * 526: FACTOR * 526, (FACTOR - 1) * 526: FACTOR * 526]\n",
    "\n",
    "transformed_arr_1 = transform_hic(arr, arr_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr_1[7:526 - 7, 7:526 - 7], chromosome_position=chromosome_position1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTOR = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pred2 = sequences['chr2R'][526000 * (FACTOR - 1):526000 * FACTOR,:].reshape(1, 526000, 4)[:, 856:(526000 - 856), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromosome_position2 = ('chr2R', (526000 * (FACTOR - 1) + 856, 526000 * FACTOR - 856))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction2 = model.predict(to_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_matrix = from_upper_triu(prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(prediction_matrix, chromosome_position=chromosome_position2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = c.matrix(balance=True).fetch('chr2R')[(FACTOR - 1) * 526: FACTOR * 526, (FACTOR - 1) * 526: FACTOR * 526]\n",
    "# For adaptive coarse-grain transformation purposes\n",
    "arr_raw = c.matrix(balance=False).fetch('chr2R')[(FACTOR - 1) * 526: FACTOR * 526, (FACTOR - 1) * 526: FACTOR * 526]\n",
    "\n",
    "transformed_arr_2 = transform_hic(arr, arr_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr_2[7:526 - 7, 7:526 - 7], chromosome_position=chromosome_position2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTOR = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pred3 = sequences['chr3L'][526000 * (FACTOR - 1):526000 * FACTOR,:].reshape(1, 526000, 4)[:, 856:(526000 - 856), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromosome_position3 = ('chr3L', (526000 * (FACTOR - 1) + 856, 526000 * FACTOR - 856))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction3 = model.predict(to_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_matrix = from_upper_triu(prediction3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(prediction_matrix, chromosome_position=chromosome_position3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = c.matrix(balance=True).fetch('chr3L')[(FACTOR - 1) * 526: FACTOR * 526, (FACTOR - 1) * 526: FACTOR * 526]\n",
    "# For adaptive coarse-grain transformation purposes\n",
    "arr_raw = c.matrix(balance=False).fetch('chr3L')[(FACTOR - 1) * 526: FACTOR * 526, (FACTOR - 1) * 526: FACTOR * 526]\n",
    "\n",
    "transformed_arr_3 = transform_hic(arr, arr_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr_3[7:526 - 7, 7:526 - 7], chromosome_position=chromosome_position3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONST = 680"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_to_predict = test_x[CONST:(CONST+1), :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(seq_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_matrix = from_upper_triu(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(prediction_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(from_upper_triu(test_y[CONST:(CONST+1), :, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_upper_triu(vector_repr, matrix_len = 512, num_diags = 2):\n",
    "    z = np.zeros((matrix_len,matrix_len))\n",
    "    triu_tup = np.triu_indices(matrix_len,num_diags)\n",
    "    z[triu_tup] = vector_repr[0, :, 0]\n",
    "    for i in range(-num_diags+1,num_diags):\n",
    "        set_diag(z, np.nan, i)\n",
    "    return z + z.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_sgd = tf.keras.optimizers.SGD(\n",
    "          lr=0.0065,\n",
    "          momentum=0.99575,\n",
    "          clipnorm=10.7)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.MSE,\n",
    "                    optimizer=optimizer_sgd,\n",
    "                    metrics=['mae']) # TODO: R^2 and Person custom metrics (? TensorBoard and EarlyStopping ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_x,\n",
    "                    train_y,\n",
    "                    batch_size=2,\n",
    "                    epochs=5,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, we implement NN without stochastic shift and hi-c matrix flip\n",
    "# this will help performance and is TODO\n",
    "# Also Fudenberg trains on 5 datasets at the same time (Multi-task training). This improves accuracy on all dataset predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убрать crop, уменьшить НС, multitask learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Fine-tuning method\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# у его one-hot-encoding - ACGT Порядок сделать у меня такой же (поменять training set)\n",
    "# change training set to 512x512\n",
    "\n",
    "\n",
    "import json\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1' ### run on CPU\n",
    "print(tf.__version__)\n",
    "if tf.__version__[0] == '1':\n",
    "    tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "from cooltools.lib.numutils import set_diag\n",
    "from basenji import dataset, dna_io, seqnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./basenji/\"\n",
    "params_file = model_dir+'params.json'\n",
    "model_file  = model_dir+'model_best.h5'\n",
    "with open(params_file) as params_open:\n",
    "    params = json.load(params_open)\n",
    "    params_model = params['model']\n",
    "    params_train = params['train']\n",
    "\n",
    "seq_length = params_model['seq_length']\n",
    "target_length = params_model['target_length']\n",
    "target_crop = params_model['target_crop']\n",
    "human_model = seqnn.SeqNN(params_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_model.restore(model_file)\n",
    "print('successfully loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_model = human_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only relevant if model was compile 1 time\n",
    "human_model.get_layer(\"conv1d_2\") # layer from which we start copying weights\n",
    "human_model.get_layer(\"batch_normalization_40\") # layer at which we stop copying weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_model_layers = []\n",
    "for layer in human_model.layers[12:-7]:\n",
    "    if 'conv1d' in layer.name or 'batch_normalization' in layer.name or 'conv2d' in layer.name:\n",
    "        human_model_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 524288\n",
    "sequence = tf.keras.Input(shape=(SEQ_LENGTH, 4), name='sequence')\n",
    "\n",
    "current = sequence\n",
    "\n",
    "# Augmentation - Enable it later (after good performance on the training set)\n",
    "# current, reverse_bool = StochasticReverseComplement()(current)\n",
    "# augment_shift = 11\n",
    "# current = StochasticShift(augment_shift)(current)\n",
    "\n",
    "# First 1D convolution\n",
    "current = conv_block(current, filters=96, kernel_size=11, pool_size=2, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "# Multiple (11) 1D convolutions in a tower to arrive to 1024bp representation in 1D vectors\n",
    "current = conv_tower(current, filters_init=96, filters_mult=1.0, kernel_size=5, pool_size=2, repeat=9, \n",
    "                     batch_norm=True, bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# Dilated residual layers\n",
    "current = dilated_residual(current, filters=48, rate_mult=1.75, repeat=8, dropout=0.4, batch_norm=True, \n",
    "                           bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# Bottleneck 1D convolution\n",
    "current = conv_block(current, filters=64, kernel_size=5, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "# final activation\n",
    "current = activate(current, \"relu\")\n",
    "\n",
    "\n",
    "# HEAD:\n",
    "current = one_to_two(current)\n",
    "current = concat_dist_2d(current)\n",
    "current = conv_block_2d(current, filters=48, kernel_size=3, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "current = symmetrize_2d(current)\n",
    "current = dilated_residual_2d(current, filters=24, kernel_size=3, rate_mult=1.75, repeat=6, dropout=0.1,\n",
    "                              batch_norm=True, bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# TODO: TRY WITHOUT CROP\n",
    "# current = cropping_2d(current, cropping=26)\n",
    "\n",
    "current = upper_tri(current, diagonal_offset=2)\n",
    "\n",
    "current = dense(current, units=1, activation=\"linear\")\n",
    "# current = dense(current, units=5, activation=\"linear\")\n",
    "\n",
    "# current = SwitchReverse()([current, reverse_bool])\n",
    "\n",
    "# make model\n",
    "drosophila_model = tf.keras.Model(inputs=sequence, outputs=current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drosophila_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_sgd = tf.keras.optimizers.SGD(\n",
    "          lr=0.0065,\n",
    "          momentum=0.99575,\n",
    "          clipnorm=10.7)\n",
    "\n",
    "drosophila_model.compile(loss=tf.keras.losses.MSE,\n",
    "                    optimizer=optimizer_sgd,\n",
    "                    metrics=['mae']) # TODO: R^2 and Person custom metrics (? TensorBoard and EarlyStopping ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drosophila_model.fit(train_x,\n",
    "                    train_y,\n",
    "                    batch_size=2,\n",
    "                    epochs=5,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "524288 / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(526000 - 524288) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "526 - 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Алгоритм\n",
    "# Берем последовательности ДНК 526.000. В нейронке (или в процессе формирования трен множества) клипаем их с двух сторон по 856 с каждой -> 524288\n",
    "# Берем Hi-C 526x526 -> клипаем по 7 c каждой стороны -> 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weights from the fudenberg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for layer in drosophila_model.layers[6:]:\n",
    "    if 'conv1d' in layer.name or 'batch_normalization' in layer.name or 'conv2d' in layer.name:\n",
    "        layer.set_weights(human_model_layers[count].get_weights())\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Data augmentation model\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 70/30 - train/test\n",
    "\n",
    "# train_indices = np.random.choice(1836, 1300, replace = False)\n",
    "# np.save(\"./train_test_data/v2/train_indices.npy\", train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FROM THE MEMORY\n",
    "train_indices = np.load(\"./train_test_data/v2/train_indices.npy\")\n",
    "test_indices = np.load(\"./train_test_data/v2/test_indices.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_indices))\n",
    "print(len(test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original X\n",
    "train_x = form_train_x(sequences, chroms, training_squares, train_indices)\n",
    "\n",
    "np.savez_compressed(\"./train_test_data/v2/train_x.npz\", train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse complement sequences\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "sequences_reverse_complemented = dict()\n",
    "\n",
    "for k,v in sequences_orig.items():\n",
    "    sequences_reverse_complemented[k] = str(Seq(v).reverse_complement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_dna(sequences_reverse_complemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse complemented\n",
    "train_x_reverse_complemented = form_train_x(sequences_reverse_complemented, chroms, training_squares, train_indices)\n",
    "\n",
    "np.savez_compressed(\"./train_test_data/v2/train_x_reverse_complemented.npz\", train_x_reverse_complemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Y\n",
    "train_y = form_train_y(training_squares, chroms, train_indices)\n",
    "\n",
    "np.savez_compressed(\"./train_test_data/v2/train_y.npz\", train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip training_squares along the diagonal\n",
    "\n",
    "import copy\n",
    "flipped_training_squares = copy.deepcopy(training_squares)\n",
    "\n",
    "for chrom in chroms:     \n",
    "    cur_training_squares = training_squares[chrom]\n",
    "\n",
    "    for i in range(len(cur_training_squares)):\n",
    "        # convert to list to allow modifications\n",
    "        flipped_training_squares[chrom][i] = list(flipped_training_squares[chrom][i])\n",
    "        \n",
    "        flipped_training_squares[chrom][i][0] = np.rot90(cur_training_squares[i][0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(training_squares['chrX'][152][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(flipped_training_squares['chrX'][152][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flipped along diagonal\n",
    "train_y_flipped = form_train_y(flipped_training_squares, chroms, train_indices)\n",
    "\n",
    "np.savez_compressed(\"./train_test_data/v2/train_y_flipped.npz\", train_y_flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Original and Reverse complement datasets into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig = np.load(\"./train_test_data/v2/train_x.npz\")['arr_0']\n",
    "train_x_reverse_complemented = np.load(\"./train_test_data/v2/train_x_reverse_complemented.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_combined = np.concatenate((train_x_orig, train_x_reverse_complemented), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_orig = np.load(\"./train_test_data/v2/train_y.npz\")['arr_0']\n",
    "train_y_flipped = np.load(\"./train_test_data/v2/train_y_flipped.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_combined = np.concatenate((train_y_orig, train_y_flipped), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle X and Y randomly\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_x_combined_shuffled, train_y_combined_shuffled = shuffle(train_x_combined, train_y_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split big dataset into 3 smaller datsets (because of Google Colab RAM capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_x = np.split(train_x_combined_shuffled[0:2058, :, :], 3)\n",
    "splitted_y = np.split(train_y_combined_shuffled[0:2058, :, :], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, we add 4th dataset which combines small parts from 3 datasets -> to adress catastrophic forgetting issue\n",
    "to_concatenate_from_x = []\n",
    "to_concatenate_from_y = []\n",
    "\n",
    "for i in range(3):\n",
    "    random_choice = np.random.choice(686, int(686/3), replace = False)\n",
    "    \n",
    "    to_concatenate_from_x.append(splitted_x[i][random_choice])\n",
    "    to_concatenate_from_y.append(splitted_y[i][random_choice])\n",
    "    \n",
    "\n",
    "splits_combination_x = np.concatenate(to_concatenate_from_x)\n",
    "splits_combination_y = np.concatenate(to_concatenate_from_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save splitted datasets\n",
    "\n",
    "for i in range(3):\n",
    "    np.savez_compressed(f\"./train_test_data/v2/splitted_dataset/train_x_part_{i}.npz\", splitted_x[i])\n",
    "    np.savez_compressed(f\"./train_test_data/v2/splitted_dataset/train_y_part_{i}.npz\", splitted_y[i])\n",
    "    \n",
    "\n",
    "np.savez_compressed(f\"./train_test_data/v2/splitted_dataset/train_x_part_4.npz\", splits_combination_x)\n",
    "np.savez_compressed(f\"./train_test_data/v2/splitted_dataset/train_y_part_4.npz\", splits_combination_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = form_train_x(sequences, chroms, training_squares, test_indices)\n",
    "test_y = form_train_y(training_squares, chroms, test_indices)\n",
    "\n",
    "np.savez_compressed(\"./train_test_data/v2/test_x.npz\", test_x)\n",
    "np.savez_compressed(\"./train_test_data/v2/test_y.npz\", test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First predictions on the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.load(\"./train_test_data/v2/test_x.npz\")['arr_0']\n",
    "test_y = np.load(\"./train_test_data/v2/test_y.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(536, 524288, 4)\n",
      "(536, 130305, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./weights/v2/model_80_epochs.h5\")\n",
    "# TODO: compare with 68 epochs model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(from_upper_triu(model.predict(test_x[0].reshape(1, 524288, 4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(from_upper_triu(model_old.predict(test_x[0].reshape(1, 524288, 4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(from_upper_triu(test_y[0].reshape(1, 130305, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(from_upper_triu(model.predict(test_x[130].reshape(1, 524288, 4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(from_upper_triu(model_old.predict(test_x[130].reshape(1, 524288, 4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(from_upper_triu(test_y[130].reshape(1, 130305, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion: New model's predictions are much more precise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze augmented model weight checkpoints on the test set\n",
    "# Pick the best weights of the augmented model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "for filename in sorted(os.listdir(\"./weights/v2\"), key=lambda filename: int(re.findall(r'\\d+', filename)[0])):\n",
    "    num_of_epochs = int(re.findall(r'\\d+', filename)[0])\n",
    "    x.append(num_of_epochs)\n",
    "    \n",
    "    model.load_weights(f\"./weights/v2/{filename}\")\n",
    "    \n",
    "    y.append(rmse(model, test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 48, 52, 56, 60, 64, 68, 72, 76, 80]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd209f95250>]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion: optimal weights: at 70 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse(model, test_x, test_y, num_samples=50):\n",
    "    avg_rmse = 0\n",
    "    for i in range(num_samples):\n",
    "        y_orig = test_y[i].tolist()\n",
    "        y_pred = model.predict(test_x[i].reshape(1, 524288, 4)).reshape(130305, 1).tolist()\n",
    "        \n",
    "        avg_rmse += mean_squared_error(y_orig, y_pred)\n",
    "\n",
    "    \n",
    "    return avg_rmse / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE analysis between old and new models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22694600232189932"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80 epochs\n",
    "rmse(model, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3803311037028989"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(model_old, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15209504571927068"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 68 epochs - best model\n",
    "model.load_weights(\"./weights/v2/model_68_epochs.h5\")\n",
    "rmse(model, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
