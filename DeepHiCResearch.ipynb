{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Notes\n",
    "\n",
    "в пределах 10мб - не мусорные контакты\n",
    "всю Hi-C карту предсказывать не надо. участки вдоль диагонали\n",
    "\n",
    "удалить главную диагональ? - схлопывать\n",
    "\n",
    "\n",
    "TODO: реализовать RMSE и R^2 для численной оценки качества предсказаний моделей\n",
    "Разделить на 80 training, 10 dev, 10 test\n",
    "Посмотреть качество предсказаний на test set'е\n",
    "\n",
    "\n",
    "TODO: \n",
    "Learn only on upper triangle (2 times less output values, should be faster). \n",
    "Then, after prediction, form 2d matrix from upper triangle\n",
    "\n",
    "TODO:\n",
    "Use all chromosomes (not only one) for training and testing NN\n",
    "\n",
    "\n",
    "TODO:\n",
    "To reduce overfitting and increase generalization\n",
    "stochastically shift input sequences by up to +/- 11 bp and reverse complement the DNA and flip the Hi-C map.\n",
    "(described in Fudenberg)\n",
    "\n",
    "\n",
    "Refactoring is needed!\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "pandas.set_option('display.max_columns', 500)\n",
    "pandas.set_option('display.max_rows', 500)\n",
    "\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import cooler\n",
    "import cooltools as ct\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "import pickle\n",
    "\n",
    "import scipy\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following directive activates inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# allow to allocate resources for model training\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import set_session\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# Training set formation\n",
    "WINDOW_SIZE = 526\n",
    "STRIDE = 526 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTIL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hic(matrix, use_log_scale = False, chromosome_position = ()):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    if use_log_scale:\n",
    "        im = ax.matshow(np.log10(matrix), cmap='YlOrRd')\n",
    "        fig.colorbar(im)\n",
    "    else:\n",
    "        im = ax.matshow(matrix, cmap='YlOrRd')\n",
    "        fig.colorbar(im)\n",
    "    \n",
    "    if len(chromosome_position) != 0:\n",
    "        ax.set_title(f\"{chromosome_position[0]}: {chromosome_position[1][0]}-{chromosome_position[1][1]}\", fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_dna(sequences = {}):\n",
    "    for k,v in sequences.items():\n",
    "        seq_array = np.array(list(v))\n",
    "\n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded_seq = label_encoder.fit_transform(seq_array)\n",
    "\n",
    "        integer_encoded_seq = integer_encoded_seq.reshape(len(integer_encoded_seq), 1)\n",
    "\n",
    "        onehot_encoder = OneHotEncoder(sparse = False)\n",
    "        result = onehot_encoder.fit_transform(integer_encoded_seq)\n",
    "        \n",
    "        # if Ns are present in the DNA sequence, result will have 5 columns. We delete 4th column which has Ns\n",
    "        # N row in the resulting training set will have all 0s\n",
    "        if result.shape[1] == 5:\n",
    "            result = np.delete(result, 3, 1)\n",
    "        \n",
    "        sequences[k] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_training_squares(hic_library, chroms):\n",
    "    training_set = {}\n",
    "\n",
    "    COLAB_OFFSET = 526 # offset from the transformation step from the Google Colab\n",
    "    for chrom in chroms:\n",
    "        current_chrom = hic_library[chrom]\n",
    "        training_set[chrom] = []\n",
    "        \n",
    "        for part_number in sorted(current_chrom.keys()):\n",
    "            current_chrom_part = current_chrom[part_number]\n",
    "            \n",
    "            offset = (part_number - 1) * COLAB_OFFSET\n",
    "            \n",
    "            for i in range(WINDOW_SIZE, current_chrom_part.shape[0] + 1, STRIDE):\n",
    "                training_set[chrom].append((current_chrom_part[i - WINDOW_SIZE:i, i - WINDOW_SIZE:i],\n",
    "                                          (offset + (i - WINDOW_SIZE), offset + i)))\n",
    "            \n",
    "    return training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_train_x(sequences_one_hot, chroms, training_squares, train_indices):\n",
    "    train_x = []\n",
    "    count = 0\n",
    "    \n",
    "    CROPPING_FACTOR_DNA = 856\n",
    "    \n",
    "    for chrom in chroms:\n",
    "        cur_seq = sequences_one_hot[chrom]\n",
    "        cur_training_squares = training_squares[chrom]\n",
    "\n",
    "        for training_square in cur_training_squares:\n",
    "            sq_begin, sq_end = training_square[1]\n",
    "            \n",
    "            if count in train_indices:\n",
    "                # crop train_x to match 524288\n",
    "                train_x.append(cur_seq[(sq_begin * 1000) + CROPPING_FACTOR_DNA:(sq_end * 1000) - CROPPING_FACTOR_DNA, :])\n",
    "\n",
    "            count += 1\n",
    "            \n",
    "    return np.asarray(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_train_y(training_squares, chroms, train_indices):\n",
    "    train_y = []\n",
    "    count = 0\n",
    "    \n",
    "    for chrom in chroms:            \n",
    "        cur_training_squares = training_squares[chrom]\n",
    "\n",
    "        for training_square in cur_training_squares:\n",
    "            CROPPING_TARGET = 7\n",
    "            cropped_training_square = training_square[0][CROPPING_TARGET:training_square[0].shape[0] - CROPPING_TARGET, \n",
    "                                                         CROPPING_TARGET:training_square[0].shape[1] - CROPPING_TARGET]\n",
    "\n",
    "            upper_triu = to_upper_triu(cropped_training_square, diagonal_offset=2)\n",
    "            \n",
    "            if count in train_indices:\n",
    "                train_y.append(upper_triu)\n",
    "                \n",
    "            count += 1   \n",
    "            \n",
    "    return np.asarray(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_upper_triu(input_matrix, diagonal_offset = 2):\n",
    "    seq_len = input_matrix.shape[0]\n",
    "    return input_matrix[np.triu_indices(seq_len, diagonal_offset)].reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab specific code\n",
    "# filepath = \"drive/My Drive/Colab Notebooks/S2-Wang2017-async.dm3.mapq_30.100.mcool\"\n",
    "\n",
    "filepath = \"S2-Wang2017-async.dm3.mapq_30.100.mcool\"\n",
    "\n",
    "resolution = \"::/resolutions/1000\" # 1 KB resolution\n",
    "c = cooler.Cooler(filepath + resolution)\n",
    "\n",
    "chroms = c.chromnames\n",
    "# don't use these small chromosomes\n",
    "chroms.remove(\"chrM\")\n",
    "chroms.remove(\"chr4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T RUN, LOAD TRANSFORMED Hi-C INSTEAD\n",
    "for chrom in chroms:\n",
    "    arr = c.matrix(balance=True).fetch(chrom)\n",
    "    # For adaptive coarse-grain transformation purposes\n",
    "    arr_raw = c.matrix(balance=False).fetch(chrom)\n",
    "    \n",
    "    transformed_arr = transform_hic(arr, arr_raw)\n",
    "    file_name = chrom + \"_transformed.npy\"\n",
    "    np.save(f\"./transformed_hic/{file_name}\", transformed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(arr, use_log_scale = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all transformed Hi-C\n",
    "transformed_hic = {}\n",
    "for chrom in chroms:\n",
    "    chrom_hic_filenames = [filename for filename in os.listdir('./transformed_hic') if filename.startswith(chrom)]\n",
    "    \n",
    "    if len(chrom_hic_filenames) > 0:\n",
    "        chrom_parts = {}\n",
    "        for filename in chrom_hic_filenames:\n",
    "            current_part_number = int(re.search(\"(transformed)(\\d+)\", filename).group(2))\n",
    "            chrom_parts[current_part_number] = np.load(f\"./transformed_hic/{filename}\")['arr_0']\n",
    "\n",
    "        transformed_hic[chrom] = chrom_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_squares = select_training_squares(transformed_hic, chroms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = {}\n",
    "for chrom in chroms:\n",
    "    fasta_sequence = list(SeqIO.parse(open(f\"./chromFa/{chrom}.fa\"),'fasta'))[0]\n",
    "    sequences[chrom] = str(fasta_sequence.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Воспринимаю маленькие и большие буквы одинаковым образом\n",
    "for k in sequences.keys():\n",
    "    sequences[k] = sequences[k].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For later use (augmentation)\n",
    "sequences_orig = sequences.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразую последовательности в one-hot encoding (A = 0, C = 1, G = 2, T = 3)\n",
    "one_hot_dna(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 60/40 - train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_indices = np.random.choice(1836, 1150, replace = False) \n",
    "# 1150 - is 60 percent of the whole dataset\n",
    "# np.save(\"./train_indices.npy\", train_indices)\n",
    "\n",
    "# LOAD FROM THE MEMORY\n",
    "train_indices = np.load(\"./train_test_data/train_indices.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.load(\"./train_test_data/train_indices.npy\")\n",
    "test_indices = np.load(\"./train_test_data/test_indices.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = form_train_x(sequences, chroms, training_squares, test_indices)\n",
    "test_y = form_train_y(training_squares, chroms, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"./test_x.npz\", test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"./test_y.npz\", test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = form_train_x(sequences, chroms, training_squares, train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"./train_x.npz\", train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = form_train_y(training_squares, chroms, train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"./train_y.npz\", train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convnet model (similar to Fudenberg NN):\n",
    "\n",
    "model_m = Sequential()\n",
    "\n",
    "model_m.add(layers.Conv1D(25, 50, activation='relu', input_shape=(50000, 4)))\n",
    "model_m.add(layers.Conv1D(25, 50, activation='relu'))\n",
    "model_m.add(layers.MaxPooling1D(5, strides = 2))\n",
    "\n",
    "model_m.add(layers.Conv1D(50, 25, activation='relu'))\n",
    "model_m.add(layers.MaxPooling1D(5, strides = 2))\n",
    "\n",
    "model_m.add(layers.Conv1D(50, 25, activation='relu'))\n",
    "model_m.add(layers.MaxPooling1D(20, strides = 4))\n",
    "\n",
    "model_m.add(layers.Conv1D(70, 20, activation='relu'))\n",
    "model_m.add(layers.MaxPooling1D(25, strides = 4))\n",
    "\n",
    "# dilated layers\n",
    "model_m.add(layers.Conv1D(100, 15, activation='relu', dilation_rate = 2))\n",
    "model_m.add(layers.Conv1D(100, 15, activation='relu', dilation_rate = 2))\n",
    "model_m.add(layers.MaxPooling1D(25, strides = 4))\n",
    "\n",
    "model_m.add(layers.Flatten())\n",
    "model_m.add(layers.Dense(2500, activation='linear'))\n",
    "\n",
    "# здесь можно не использовать входы (50 штук) и схлопывать после предсказания НС\n",
    "# не 2.500, а 2.450\n",
    "# для тренировки и для предсказаний надо будет сначала делать трансформацию для вектора\n",
    "\n",
    "print(model_m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
    "              loss='mse',\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_m.fit(train_x,\n",
    "                      train_y,\n",
    "                      batch_size=32,\n",
    "                      epochs=3,\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.load_weights('project2_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(training_squares[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_begin, sq_end = training_squares[0][1]\n",
    "seq_to_use = current_seq_one_hot[(sq_begin * 1000):(sq_end * 1000), ].reshape((1,50000, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model_m.predict(seq_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(prediction.reshape((50, 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.load_weights('project2_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(training_squares[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_begin, sq_end = training_squares[3][1]\n",
    "seq_to_use = current_seq_one_hot[(sq_begin * 1000):(sq_end * 1000), ].reshape((1,50000, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model_m.predict(seq_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(prediction.reshape((50, 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fudenberg избавляется от scaling'а\n",
    "# избавиться от шкалирования. полимерное свойство хроматина нам не обязательно выучивать\n",
    "# Нам надо это делать:\n",
    "# observed/expected - Саша отправит\n",
    "# можно применять observed/expected для всей хромосомы или для каждого квардрата 50x50\n",
    "# для каждого квардрата делать observed/expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Басет работа - 2015 (размеры слоев и параметры оттуда)\n",
    "# В басет предсказывается DNA sequence -> accessibility (open versus closed chromatin) of this area \n",
    "# (in different cell types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convnet model (similar to Basset NN):\n",
    "# Basset NN is not trainable in a reasonable time with this 50x50 window size -> reduced to 25x25 window size\n",
    "# Nevertheless, 3 times more tunable parameters than Fudenberg\n",
    "\n",
    "model_m = Sequential()\n",
    "\n",
    "model_m.add(layers.Conv1D(300, 21, activation='relu', input_shape=(25000, 4)))\n",
    "model_m.add(layers.MaxPooling1D(4))\n",
    "\n",
    "model_m.add(layers.Conv1D(300, 6, activation='relu'))\n",
    "model_m.add(layers.MaxPooling1D(4))\n",
    "\n",
    "model_m.add(layers.Conv1D(500, 4, activation='relu'))\n",
    "model_m.add(layers.MaxPooling1D(4))\n",
    "\n",
    "model_m.add(layers.Flatten())\n",
    "\n",
    "model_m.add(layers.Dense(1000, activation='relu'))\n",
    "\n",
    "model_m.add(layers.Dense(625, activation='linear'))\n",
    "\n",
    "print(model_m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.load_weights('project2_model_Basset.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(training_squares[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_begin, sq_end = training_squares[3][1]\n",
    "seq_to_use = current_seq_one_hot[(sq_begin * 1000):(sq_end * 1000), ].reshape((1,25000, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model_m.predict(seq_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(prediction.reshape((25, 25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hi-C tranformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(arr[0:1000, 0:1000], use_log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Adaptive coarse-grain\n",
    "transformed_arr = ct.lib.numutils.adaptive_coarsegrain(arr[0:1000, 0:1000], arr_raw[0:1000, 0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr, use_log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Normalize the contact matrix for distance-dependent contact decay.\n",
    "# Observed/Expected\n",
    "transformed_arr, _, _, _ = ct.lib.numutils.observed_over_expected(transformed_arr, mask = ~np.isnan(transformed_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr, use_log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Take natural logarithm\n",
    "transformed_arr = np.log(transformed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Interpolate all NaN values\n",
    "transformed_arr = ct.lib.numutils.interp_nan(transformed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Use Gaussian filter\n",
    "# To get rid of noise, emphasizing larger patterns.\n",
    "transformed_arr = scipy.ndimage.gaussian_filter(transformed_arr, sigma = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the transformations in one function\n",
    "def transform_hic(hic_matrix, hic_matrix_raw):\n",
    "    transformed_arr = ct.lib.numutils.adaptive_coarsegrain(hic_matrix, hic_matrix_raw)\n",
    "    transformed_arr, _, _, _ = ct.lib.numutils.observed_over_expected(transformed_arr, mask = ~np.isnan(transformed_arr))\n",
    "    transformed_arr = np.log(transformed_arr)\n",
    "    transformed_arr = ct.lib.numutils.interp_nan(transformed_arr)\n",
    "    transformed_arr = scipy.ndimage.gaussian_filter(transformed_arr, sigma = 1)\n",
    "    \n",
    "    return transformed_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(arr[0:4000, 0:4000], use_log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transform_hic(arr[2500:3000, 2500:3000], arr_raw[2500:3000, 2500:3000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: RMSE and R^2. split on train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (None, 5, 3) -> (None, 5, 5, 3)\n",
    "# Explanation of the 1D -> 2D procedure\n",
    "\n",
    "oned = tf.constant([[[1, 6, 11], \n",
    "                     [2, 7, 12], \n",
    "                     [3, 8, 13],\n",
    "                     [4, 9, 14],\n",
    "                     [5, 10, 15]],\n",
    "                   \n",
    "                    [[1, 6, 11], \n",
    "                     [2, 7, 12], \n",
    "                     [3, 8, 13],\n",
    "                     [4, 9, 14],\n",
    "                     [5, 10, 15]],\n",
    "                   \n",
    "                    [[1, 6, 11], \n",
    "                     [2, 7, 12], \n",
    "                     [3, 8, 13],\n",
    "                     [4, 9, 14],\n",
    "                     [5, 10, 15]]])\n",
    "\n",
    "_, seq_len, features = oned.shape\n",
    "twod1 = tf.tile(oned, [1, seq_len, 1])\n",
    "twod1 = tf.reshape(twod1, [-1, seq_len, seq_len, features])\n",
    "twod2 = tf.transpose(twod1, [0,2,1,3])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Original tiled tensor for one 1D filter (1D filter expanded 5 times)\")\n",
    "    print(np.array(sess.run([twod1]))[0][0][:, :, 0])\n",
    "    \n",
    "    print(\"Transposed tiled tensor\")\n",
    "    print(np.array(sess.run([twod2]))[0][0][:, :, 0])\n",
    "\n",
    "\n",
    "twod1 = tf.expand_dims(twod1, axis=-1)\n",
    "twod2 = tf.expand_dims(twod2, axis=-1)\n",
    "twod  = tf.concat([twod1, twod2], axis=-1)\n",
    "twod = tf.reduce_mean(twod, axis=-1)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Result of mean operation between original and transposed (mean between each pair of values in 1D filter)\")\n",
    "    print(np.array(sess.run([twod]))[0][0][:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exlanation of ConcatDist2D procedure\n",
    "# This layer adds one more channel which contains pairwise distances for the matrices obtained on the previous layer\n",
    "# (this should enhance model performance)\n",
    "# (None, 5, 5, 3) -> (None, 5, 5, 4)\n",
    "\n",
    "# For one training example -> (1,5,5,3)\n",
    "inputs = tf.random.uniform(shape=[1,5,5,3])\n",
    "\n",
    "input_shape = tf.shape(inputs)\n",
    "batch_size, seq_len = input_shape[0], input_shape[1]\n",
    "\n",
    "## concat 2D distance ##\n",
    "pos = tf.expand_dims(tf.range(0, seq_len), axis=-1)\n",
    "matrix_repr1 = tf.tile(pos, [1,seq_len])\n",
    "matrix_repr2 = tf.transpose(matrix_repr1, [1,0])\n",
    "dist  = tf.math.abs( tf.math.subtract(matrix_repr1, matrix_repr2) )\n",
    "dist = tf.dtypes.cast(dist, tf.float32)\n",
    "dist = tf.expand_dims(dist, axis=-1)\n",
    "dist = tf.expand_dims(dist, axis=0)\n",
    "dist = tf.tile(dist, [batch_size, 1, 1, 1])\n",
    "\n",
    "res = tf.concat([inputs, dist], axis=-1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(np.array(sess.run([res]))[0, :, :, :, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Custom Keras layers\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneToTwo(tf.keras.layers.Layer):\n",
    "    ''' Transform 1d to 2d with i,j vectors operated on.'''\n",
    "    def __init__(self, operation='mean'):\n",
    "        super(OneToTwo, self).__init__()\n",
    "\n",
    "    def call(self, oned):\n",
    "        _, seq_len, features = oned.shape\n",
    "\n",
    "        twod1 = tf.tile(oned, [1, seq_len, 1])\n",
    "        twod1 = tf.reshape(twod1, [-1, seq_len, seq_len, features])\n",
    "        twod2 = tf.transpose(twod1, [0,2,1,3])\n",
    "\n",
    "        twod1 = tf.expand_dims(twod1, axis=-1)\n",
    "        twod2 = tf.expand_dims(twod2, axis=-1)\n",
    "        twod  = tf.concat([twod1, twod2], axis=-1)\n",
    "        twod = tf.reduce_mean(twod, axis=-1)\n",
    "\n",
    "        return twod\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config['operation'] = self.operation\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDist2D(tf.keras.layers.Layer):\n",
    "    ''' Concatenate the pairwise distance to 2d feature matrix.'''\n",
    "    def __init__(self):\n",
    "        super(ConcatDist2D, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, seq_len = input_shape[0], input_shape[1]\n",
    "\n",
    "        ## concat 2D distance ##\n",
    "        pos = tf.expand_dims(tf.range(0, seq_len), axis=-1)\n",
    "        matrix_repr1 = tf.tile(pos, [1, seq_len])\n",
    "        matrix_repr2 = tf.transpose(matrix_repr1, [1, 0])\n",
    "        dist = tf.math.abs(tf.math.subtract(matrix_repr1, matrix_repr2))\n",
    "        dist = tf.dtypes.cast(dist, tf.float32)\n",
    "        dist = tf.expand_dims(dist, axis=-1)\n",
    "        dist = tf.expand_dims(dist, axis=0)\n",
    "        dist = tf.tile(dist, [batch_size, 1, 1, 1])\n",
    "        return tf.concat([inputs, dist], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Symmetrize2D(tf.keras.layers.Layer):\n",
    "    '''Take the average of a matrix and its transpose to enforce symmetry.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Symmetrize2D, self).__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        x_t = tf.transpose(x, [0, 2, 1, 3])\n",
    "        x_sym = (x + x_t) / 2\n",
    "        return x_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpperTri(tf.keras.layers.Layer):\n",
    "    ''' Unroll matrix to its upper triangular portion.'''\n",
    "\n",
    "    def __init__(self, diagonal_offset=2):\n",
    "        super(UpperTri, self).__init__()\n",
    "        self.diagonal_offset = diagonal_offset\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_len = inputs.shape[1].value\n",
    "        output_dim = inputs.shape[-1]\n",
    "\n",
    "        triu_tup = np.triu_indices(seq_len, self.diagonal_offset)\n",
    "        triu_index = list(triu_tup[0] + seq_len * triu_tup[1])\n",
    "        unroll_repr = tf.reshape(inputs, [-1, seq_len ** 2, output_dim])\n",
    "        return tf.gather(unroll_repr, triu_index, axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config['diagonal_offset'] = self.diagonal_offset\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticReverseComplement(tf.keras.layers.Layer):\n",
    "    \"\"\"Stochastically reverse complement a one hot encoded DNA sequence.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(StochasticReverseComplement, self).__init__()\n",
    "\n",
    "    def call(self, seq_1hot, training=None):\n",
    "        if training:\n",
    "            rc_seq_1hot = tf.gather(seq_1hot, [3, 2, 1, 0], axis=-1)\n",
    "            rc_seq_1hot = tf.reverse(rc_seq_1hot, axis=[1])\n",
    "            reverse_bool = tf.random.uniform(shape=[]) > 0.5\n",
    "            src_seq_1hot = tf.cond(reverse_bool, lambda: rc_seq_1hot, lambda: seq_1hot)\n",
    "            return src_seq_1hot, reverse_bool\n",
    "        else:\n",
    "            return seq_1hot, tf.constant(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticShift(tf.keras.layers.Layer):\n",
    "    \"\"\"Stochastically shift a one hot encoded DNA sequence.\"\"\"\n",
    "\n",
    "    def __init__(self, shift_max=0, pad='uniform'):\n",
    "        super(StochasticShift, self).__init__()\n",
    "        self.shift_max = shift_max\n",
    "        self.augment_shifts = tf.range(-self.shift_max, self.shift_max + 1)\n",
    "        self.pad = pad\n",
    "\n",
    "    def call(self, seq_1hot, training=None):\n",
    "        if training:\n",
    "            shift_i = tf.random.uniform(shape=[], minval=0, dtype=tf.int64,\n",
    "                                        maxval=len(self.augment_shifts))\n",
    "            shift = tf.gather(self.augment_shifts, shift_i)\n",
    "            sseq_1hot = tf.cond(tf.not_equal(shift, 0),\n",
    "                                lambda: shift_sequence(seq_1hot, shift),\n",
    "                                lambda: seq_1hot)\n",
    "            return sseq_1hot\n",
    "        else:\n",
    "            return seq_1hot\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'shift_max': self.shift_max,\n",
    "            'pad': self.pad\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "def shift_sequence(seq, shift, pad_value=0.25):\n",
    "    \"\"\"Shift a sequence left or right by shift_amount.\n",
    "    Args:\n",
    "    seq: [batch_size, seq_length, seq_depth] sequence\n",
    "    shift: signed shift value (tf.int32 or int)\n",
    "    pad_value: value to fill the padding (primitive or scalar tf.Tensor)\n",
    "    \"\"\"\n",
    "    if seq.shape.ndims != 3:\n",
    "        raise ValueError('input sequence should be rank 3')\n",
    "    input_shape = seq.shape\n",
    "\n",
    "    pad = pad_value * tf.ones_like(seq[:, 0:tf.abs(shift), :])\n",
    "\n",
    "    def _shift_right(_seq):\n",
    "        # shift is positive\n",
    "        sliced_seq = _seq[:, :-shift:, :]\n",
    "        return tf.concat([pad, sliced_seq], axis=1)\n",
    "\n",
    "    def _shift_left(_seq):\n",
    "        # shift is negative\n",
    "        sliced_seq = _seq[:, -shift:, :]\n",
    "        return tf.concat([sliced_seq, pad], axis=1)\n",
    "\n",
    "    sseq = tf.cond(tf.greater(shift, 0),\n",
    "                   lambda: _shift_right(seq),\n",
    "                   lambda: _shift_left(seq))\n",
    "    sseq.set_shape(input_shape)\n",
    "\n",
    "    return sseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchReverse(tf.keras.layers.Layer):\n",
    "    \"\"\"Reverse predictions if the inputs were reverse complemented.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SwitchReverse, self).__init__()\n",
    "\n",
    "    def call(self, x_reverse):\n",
    "        x = x_reverse[0]\n",
    "        reverse = x_reverse[1]\n",
    "\n",
    "        xd = len(x.shape)\n",
    "        if xd == 3:\n",
    "            rev_axes = [1]\n",
    "        elif xd == 4:\n",
    "            rev_axes = [1, 2]\n",
    "        else:\n",
    "            raise ValueError('Cannot recognize SwitchReverse input dimensions %d.' % xd)\n",
    "\n",
    "        return tf.keras.backend.switch(reverse,\n",
    "                                       tf.reverse(x, axis=rev_axes),\n",
    "                                       x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Helper functions\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(current, activation, verbose=False):\n",
    "    if verbose: \n",
    "        print('activate:',activation)\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        current = tf.keras.layers.ReLU()(current)\n",
    "    elif activation == 'gelu':\n",
    "        current = GELU()(current)\n",
    "    elif activation == 'sigmoid':\n",
    "        current = tf.keras.layers.Activation('sigmoid')(current)\n",
    "    elif activation == 'tanh':\n",
    "        current = tf.keras.layers.Activation('tanh')(current)\n",
    "    elif activation == 'exp':\n",
    "        current = Exp()(current)\n",
    "    elif activation == 'softplus':\n",
    "        current = Softplus()(current)\n",
    "    else:\n",
    "        print('Unrecognized activation \"%s\"' % activation, file=sys.stderr)\n",
    "        exit(1)\n",
    "\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Keras blocks\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inputs, filters=None, kernel_size=1, activation='relu', strides=1,\n",
    "    dilation_rate=1, l2_scale=0, dropout=0, conv_type='standard', residual=False,\n",
    "    pool_size=1, batch_norm=False, bn_momentum=0.99, bn_gamma=None,\n",
    "    kernel_initializer='he_normal'):\n",
    "  \n",
    "    \"\"\"Construct a single convolution block.\n",
    "    Args:\n",
    "    inputs:        [batch_size, seq_length, features] input sequence\n",
    "    filters:       Conv1D filters\n",
    "    kernel_size:   Conv1D kernel_size\n",
    "    activation:    relu/gelu/etc\n",
    "    strides:       Conv1D strides\n",
    "    dilation_rate: Conv1D dilation rate\n",
    "    l2_scale:      L2 regularization weight.\n",
    "    dropout:       Dropout rate probability\n",
    "    conv_type:     Conv1D layer type\n",
    "    residual:      Residual connection boolean\n",
    "    pool_size:     Max pool width\n",
    "    batch_norm:    Apply batch normalization\n",
    "    bn_momentum:   BatchNorm momentum\n",
    "    bn_gamma:      BatchNorm gamma (defaults according to residual)\n",
    "    Returns:\n",
    "    [batch_size, seq_length, features] output sequence\n",
    "    \"\"\"\n",
    "\n",
    "    # flow through variable current\n",
    "    current = inputs\n",
    "\n",
    "    # choose convolution type\n",
    "    if conv_type == 'separable':\n",
    "        conv_layer = tf.keras.layers.SeparableConv1D\n",
    "    else:\n",
    "        conv_layer = tf.keras.layers.Conv1D\n",
    "\n",
    "    if filters is None:\n",
    "        filters = inputs.shape[-1]\n",
    "\n",
    "    # activation\n",
    "    current = activate(current, activation)\n",
    "\n",
    "    # convolution\n",
    "    current = conv_layer(\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        use_bias=False,\n",
    "        dilation_rate=dilation_rate,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(l2_scale))(current)\n",
    "\n",
    "    # batch norm\n",
    "    if batch_norm:\n",
    "        if bn_gamma is None:\n",
    "            bn_gamma = 'zeros' if residual else 'ones'\n",
    "        \n",
    "        current = tf.keras.layers.BatchNormalization(momentum=bn_momentum, gamma_initializer=bn_gamma, fused=True)(current)\n",
    "\n",
    "    # dropout\n",
    "    if dropout > 0:\n",
    "        current = tf.keras.layers.Dropout(rate=dropout)(current)\n",
    "\n",
    "    # residual add\n",
    "    if residual:\n",
    "        current = tf.keras.layers.Add()([inputs,current])\n",
    "\n",
    "    # Pool\n",
    "    if pool_size > 1:\n",
    "        current = tf.keras.layers.MaxPool1D(pool_size=pool_size, padding='same')(current)\n",
    "\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_tower(inputs, filters_init, filters_mult=1, repeat=1, **kwargs):\n",
    "    \"\"\"Construct a reducing convolution block.\n",
    "    Args:\n",
    "    inputs:        [batch_size, seq_length, features] input sequence\n",
    "    filters_init:  Initial Conv1D filters\n",
    "    filters_mult:  Multiplier for Conv1D filters\n",
    "    repeat:        Conv block repetitions\n",
    "    Returns:\n",
    "    [batch_size, seq_length, features] output sequence\n",
    "    \"\"\"\n",
    "\n",
    "    # flow through variable current\n",
    "    current = inputs\n",
    "\n",
    "    # initialize filters\n",
    "    rep_filters = filters_init\n",
    "\n",
    "    for ri in range(repeat):\n",
    "        # convolution\n",
    "        current = conv_block(current, filters=int(np.round(rep_filters)), **kwargs)\n",
    "\n",
    "    # update filters\n",
    "    rep_filters *= filters_mult\n",
    "\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilated_residual(inputs, filters, kernel_size=3, rate_mult=2, conv_type='standard', \n",
    "                     dropout=0, repeat=1, round=False, **kwargs):\n",
    "    \"\"\"Construct a residual dilated convolution block.\n",
    "    Args:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "\n",
    "    # flow through variable current\n",
    "    current = inputs\n",
    "\n",
    "    # initialize dilation rate\n",
    "    dilation_rate = 1.0\n",
    "\n",
    "    for ri in range(repeat):\n",
    "        # For skip connection purpose\n",
    "        rep_input = current\n",
    "\n",
    "        # dilate\n",
    "        current = conv_block(current, filters=filters, kernel_size=kernel_size, \n",
    "                             dilation_rate=int(np.round(dilation_rate)), \n",
    "                             conv_type=conv_type, bn_gamma='ones', **kwargs)\n",
    "\n",
    "        # return\n",
    "        current = conv_block(current, filters=int(rep_input.shape[-1]), dropout=dropout, bn_gamma='zeros', **kwargs)\n",
    "\n",
    "        # residual add\n",
    "        current = tf.keras.layers.Add()([rep_input, current])\n",
    "\n",
    "        # update dilation rate\n",
    "        dilation_rate *= rate_mult\n",
    "        \n",
    "        if round:\n",
    "            dilation_rate = np.round(dilation_rate)\n",
    "\n",
    "    return current\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D related blocks\n",
    "\n",
    "def concat_dist_2d(inputs, **kwargs):\n",
    "    current = ConcatDist2D()(inputs)\n",
    "    return current\n",
    "\n",
    "def one_to_two(inputs, operation='mean', **kwargs):\n",
    "    current = OneToTwo(operation)(inputs)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block_2d(inputs, filters=128, activation='relu', conv_type='standard',\n",
    "    kernel_size=1, strides=1, dilation_rate=1, l2_scale=0, dropout=0, pool_size=1,\n",
    "    batch_norm=False, bn_momentum=0.99, bn_gamma='ones', symmetric=False):\n",
    "\n",
    "    \"\"\"Construct a single 2D convolution block.   \"\"\"\n",
    "\n",
    "    # flow through variable current\n",
    "    current = inputs\n",
    "\n",
    "    # activation\n",
    "    current = activate(current, activation)\n",
    "\n",
    "  # choose convolution type\n",
    "    if conv_type == 'separable':\n",
    "        conv_layer = tf.keras.layers.SeparableConv2D\n",
    "    else:\n",
    "        conv_layer = tf.keras.layers.Conv2D\n",
    "\n",
    "    # convolution\n",
    "    current = conv_layer(\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        use_bias=False,\n",
    "        dilation_rate=dilation_rate,\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(l2_scale))(current)\n",
    "\n",
    "    # batch norm\n",
    "    if batch_norm:\n",
    "        current = tf.keras.layers.BatchNormalization(\n",
    "          momentum=bn_momentum,\n",
    "          gamma_initializer=bn_gamma,\n",
    "          fused=True)(current)\n",
    "\n",
    "    # dropout\n",
    "    if dropout > 0:\n",
    "        current = tf.keras.layers.Dropout(rate=dropout)(current)\n",
    "\n",
    "    # pool\n",
    "    if pool_size > 1:\n",
    "        current = tf.keras.layers.MaxPool2D(\n",
    "          pool_size=pool_size,\n",
    "          padding='same')(current)\n",
    "\n",
    "    # symmetric\n",
    "    if symmetric:\n",
    "        current = layers.Symmetrize2D()(current)\n",
    "\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetrize_2d(inputs, **kwargs):\n",
    "    return Symmetrize2D()(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilated_residual_2d(inputs, filters, kernel_size=3, rate_mult=2,\n",
    "                        dropout=0, repeat=1, symmetric=True, **kwargs):\n",
    "    \"\"\"Construct a residual dilated convolution block.\n",
    "    \"\"\"\n",
    "\n",
    "    # flow through variable current\n",
    "    current = inputs\n",
    "\n",
    "    # initialize dilation rate\n",
    "    dilation_rate = 1.0\n",
    "\n",
    "    for ri in range(repeat):\n",
    "        rep_input = current\n",
    "\n",
    "        # dilate\n",
    "        current = conv_block_2d(current,\n",
    "                                filters=filters,\n",
    "                                kernel_size=kernel_size,\n",
    "                                dilation_rate=int(np.round(dilation_rate)),\n",
    "                                bn_gamma='ones',\n",
    "                                **kwargs)\n",
    "\n",
    "        # return\n",
    "        current = conv_block_2d(current,\n",
    "                                filters=int(rep_input.shape[-1]),\n",
    "                                dropout=dropout,\n",
    "                                bn_gamma='zeros',\n",
    "                                **kwargs)\n",
    "\n",
    "        # residual add\n",
    "        current = tf.keras.layers.Add()([rep_input, current])\n",
    "\n",
    "        # enforce symmetry\n",
    "        if symmetric:\n",
    "            current = Symmetrize2D()(current)\n",
    "\n",
    "        # update dilation rate\n",
    "        dilation_rate *= rate_mult\n",
    "\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropping_2d(inputs, cropping, **kwargs):\n",
    "    current = tf.keras.layers.Cropping2D(cropping)(inputs)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_tri(inputs, diagonal_offset=2, **kwargs):\n",
    "    current = UpperTri(diagonal_offset)(inputs)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(inputs, units, activation='softplus', kernel_initializer='he_normal',\n",
    "          l2_scale=0, l1_scale=0, **kwargs):\n",
    "    \n",
    "    current = tf.keras.layers.Dense(\n",
    "        units=units,\n",
    "        activation=activation,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        kernel_regularizer=tf.keras.regularizers.l1_l2(l1_scale, l2_scale)\n",
    "    )(inputs)\n",
    "    \n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "\n",
    "# Changed for SEQ_LENGTH = 512000\n",
    "\n",
    "#SEQ_LENGTH = 512000\n",
    "SEQ_LENGTH = 524288\n",
    "sequence = tf.keras.Input(shape=(SEQ_LENGTH, 4), name='sequence')\n",
    "\n",
    "current = sequence\n",
    "\n",
    "# Augmentation - Enable it later (after good performance on the training set)\n",
    "# current, reverse_bool = StochasticReverseComplement()(current)\n",
    "# augment_shift = 11\n",
    "# current = StochasticShift(augment_shift)(current)\n",
    "\n",
    "# TRUNK:\n",
    "\n",
    "# First 1D convolution\n",
    "current = conv_block(current, filters=96, kernel_size=11, pool_size=2, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "\n",
    "# Change for repeat = 9\n",
    "\n",
    "# Multiple (11) 1D convolutions in a tower to arrive to 2048bp representation in 1D vectors\n",
    "current = conv_tower(current, filters_init=96, filters_mult=1.0, kernel_size=5, pool_size=2, repeat=9, \n",
    "                     batch_norm=True, bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# Dilated residual layers\n",
    "current = dilated_residual(current, filters=48, rate_mult=1.75, repeat=8, dropout=0.4, batch_norm=True, \n",
    "                           bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# Bottleneck 1D convolution\n",
    "current = conv_block(current, filters=64, kernel_size=5, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "# final activation\n",
    "current = activate(current, \"relu\")\n",
    "\n",
    "\n",
    "# HEAD:\n",
    "current = one_to_two(current)\n",
    "current = concat_dist_2d(current)\n",
    "current = conv_block_2d(current, filters=48, kernel_size=3, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "current = symmetrize_2d(current)\n",
    "current = dilated_residual_2d(current, filters=24, kernel_size=3, rate_mult=1.75, repeat=6, dropout=0.1,\n",
    "                              batch_norm=True, bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# TODO: TRY WITHOUT CROP\n",
    "#current = cropping_2d(current, cropping=26)\n",
    "\n",
    "current = upper_tri(current, diagonal_offset=2)\n",
    "\n",
    "current = dense(current, units=1, activation=\"linear\")\n",
    "#current = dense(current, units=5, activation=\"linear\")\n",
    "\n",
    "# current = SwitchReverse()([current, reverse_bool])\n",
    "\n",
    "# make model\n",
    "model = tf.keras.Model(inputs=sequence, outputs=current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./model_weights_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to predict on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = form_train_x(sequences, chroms, training_squares, test_indices)\n",
    "test_y = form_train_y(training_squares, chroms, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cooltools.lib.numutils import set_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTOR = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pred1 = sequences['chr2L'][526000 * (FACTOR - 1):526000 * FACTOR,:].reshape(1, 526000, 4)[:, 856:(526000 - 856), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromosome_position1 = ('chr2L', (526000 * (FACTOR - 1) + 856, 526000 * FACTOR - 856))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction1 = model.predict(to_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_matrix = from_upper_triu(prediction1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(prediction_matrix, chromosome_position=chromosome_position1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = c.matrix(balance=True).fetch('chr2L')[(FACTOR - 1) * 526: FACTOR * 526, (FACTOR - 1) * 526: FACTOR * 526]\n",
    "# For adaptive coarse-grain transformation purposes\n",
    "arr_raw = c.matrix(balance=False).fetch('chr2L')[(FACTOR - 1) * 526: FACTOR * 526, (FACTOR - 1) * 526: FACTOR * 526]\n",
    "\n",
    "transformed_arr_1 = transform_hic(arr, arr_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr_1[7:526 - 7, 7:526 - 7], chromosome_position=chromosome_position1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTOR = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pred2 = sequences['chr2R'][526000 * (FACTOR - 1):526000 * FACTOR,:].reshape(1, 526000, 4)[:, 856:(526000 - 856), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromosome_position2 = ('chr2R', (526000 * (FACTOR - 1) + 856, 526000 * FACTOR - 856))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction2 = model.predict(to_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_matrix = from_upper_triu(prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(prediction_matrix, chromosome_position=chromosome_position2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = c.matrix(balance=True).fetch('chr2R')[(FACTOR - 1) * 526: FACTOR * 526, (FACTOR - 1) * 526: FACTOR * 526]\n",
    "# For adaptive coarse-grain transformation purposes\n",
    "arr_raw = c.matrix(balance=False).fetch('chr2R')[(FACTOR - 1) * 526: FACTOR * 526, (FACTOR - 1) * 526: FACTOR * 526]\n",
    "\n",
    "transformed_arr_2 = transform_hic(arr, arr_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr_2[7:526 - 7, 7:526 - 7], chromosome_position=chromosome_position2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTOR = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pred3 = sequences['chr3L'][526000 * (FACTOR - 1):526000 * FACTOR,:].reshape(1, 526000, 4)[:, 856:(526000 - 856), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromosome_position3 = ('chr3L', (526000 * (FACTOR - 1) + 856, 526000 * FACTOR - 856))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction3 = model.predict(to_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_matrix = from_upper_triu(prediction3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(prediction_matrix, chromosome_position=chromosome_position3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = c.matrix(balance=True).fetch('chr3L')[(FACTOR - 1) * 526: FACTOR * 526, (FACTOR - 1) * 526: FACTOR * 526]\n",
    "# For adaptive coarse-grain transformation purposes\n",
    "arr_raw = c.matrix(balance=False).fetch('chr3L')[(FACTOR - 1) * 526: FACTOR * 526, (FACTOR - 1) * 526: FACTOR * 526]\n",
    "\n",
    "transformed_arr_3 = transform_hic(arr, arr_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr_3[7:526 - 7, 7:526 - 7], chromosome_position=chromosome_position3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONST = 680"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_to_predict = test_x[CONST:(CONST+1), :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(seq_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_matrix = from_upper_triu(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(prediction_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(from_upper_triu(test_y[CONST:(CONST+1), :, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_upper_triu(vector_repr, matrix_len = 512, num_diags = 2):\n",
    "    z = np.zeros((matrix_len,matrix_len))\n",
    "    triu_tup = np.triu_indices(matrix_len,num_diags)\n",
    "    z[triu_tup] = vector_repr[0, :, 0]\n",
    "    for i in range(-num_diags+1,num_diags):\n",
    "        set_diag(z, np.nan, i)\n",
    "    return z + z.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_sgd = tf.keras.optimizers.SGD(\n",
    "          lr=0.0065,\n",
    "          momentum=0.99575,\n",
    "          clipnorm=10.7)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.MSE,\n",
    "                    optimizer=optimizer_sgd,\n",
    "                    metrics=['mae']) # TODO: R^2 and Person custom metrics (? TensorBoard and EarlyStopping ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_x,\n",
    "                    train_y,\n",
    "                    batch_size=2,\n",
    "                    epochs=5,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, we implement NN without stochastic shift and hi-c matrix flip\n",
    "# this will help performance and is TODO\n",
    "# Also Fudenberg trains on 5 datasets at the same time (Multi-task training). This improves accuracy on all dataset predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убрать crop, уменьшить НС, multitask learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Fine-tuning method\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# у его one-hot-encoding - ACGT Порядок сделать у меня такой же (поменять training set)\n",
    "# change training set to 512x512\n",
    "\n",
    "\n",
    "import json\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1' ### run on CPU\n",
    "print(tf.__version__)\n",
    "if tf.__version__[0] == '1':\n",
    "    tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "from cooltools.lib.numutils import set_diag\n",
    "from basenji import dataset, dna_io, seqnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./basenji/\"\n",
    "params_file = model_dir+'params.json'\n",
    "model_file  = model_dir+'model_best.h5'\n",
    "with open(params_file) as params_open:\n",
    "    params = json.load(params_open)\n",
    "    params_model = params['model']\n",
    "    params_train = params['train']\n",
    "\n",
    "seq_length = params_model['seq_length']\n",
    "target_length = params_model['target_length']\n",
    "target_crop = params_model['target_crop']\n",
    "human_model = seqnn.SeqNN(params_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_model.restore(model_file)\n",
    "print('successfully loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_model = human_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only relevant if model was compile 1 time\n",
    "human_model.get_layer(\"conv1d_2\") # layer from which we start copying weights\n",
    "human_model.get_layer(\"batch_normalization_40\") # layer at which we stop copying weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_model_layers = []\n",
    "for layer in human_model.layers[12:-7]:\n",
    "    if 'conv1d' in layer.name or 'batch_normalization' in layer.name or 'conv2d' in layer.name:\n",
    "        human_model_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 524288\n",
    "sequence = tf.keras.Input(shape=(SEQ_LENGTH, 4), name='sequence')\n",
    "\n",
    "current = sequence\n",
    "\n",
    "# Augmentation - Enable it later (after good performance on the training set)\n",
    "# current, reverse_bool = StochasticReverseComplement()(current)\n",
    "# augment_shift = 11\n",
    "# current = StochasticShift(augment_shift)(current)\n",
    "\n",
    "# First 1D convolution\n",
    "current = conv_block(current, filters=96, kernel_size=11, pool_size=2, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "# Multiple (11) 1D convolutions in a tower to arrive to 1024bp representation in 1D vectors\n",
    "current = conv_tower(current, filters_init=96, filters_mult=1.0, kernel_size=5, pool_size=2, repeat=9, \n",
    "                     batch_norm=True, bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# Dilated residual layers\n",
    "current = dilated_residual(current, filters=48, rate_mult=1.75, repeat=8, dropout=0.4, batch_norm=True, \n",
    "                           bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# Bottleneck 1D convolution\n",
    "current = conv_block(current, filters=64, kernel_size=5, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "# final activation\n",
    "current = activate(current, \"relu\")\n",
    "\n",
    "\n",
    "# HEAD:\n",
    "current = one_to_two(current)\n",
    "current = concat_dist_2d(current)\n",
    "current = conv_block_2d(current, filters=48, kernel_size=3, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "current = symmetrize_2d(current)\n",
    "current = dilated_residual_2d(current, filters=24, kernel_size=3, rate_mult=1.75, repeat=6, dropout=0.1,\n",
    "                              batch_norm=True, bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# TODO: TRY WITHOUT CROP\n",
    "# current = cropping_2d(current, cropping=26)\n",
    "\n",
    "current = upper_tri(current, diagonal_offset=2)\n",
    "\n",
    "current = dense(current, units=1, activation=\"linear\")\n",
    "# current = dense(current, units=5, activation=\"linear\")\n",
    "\n",
    "# current = SwitchReverse()([current, reverse_bool])\n",
    "\n",
    "# make model\n",
    "drosophila_model = tf.keras.Model(inputs=sequence, outputs=current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drosophila_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_sgd = tf.keras.optimizers.SGD(\n",
    "          lr=0.0065,\n",
    "          momentum=0.99575,\n",
    "          clipnorm=10.7)\n",
    "\n",
    "drosophila_model.compile(loss=tf.keras.losses.MSE,\n",
    "                    optimizer=optimizer_sgd,\n",
    "                    metrics=['mae']) # TODO: R^2 and Person custom metrics (? TensorBoard and EarlyStopping ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drosophila_model.fit(train_x,\n",
    "                    train_y,\n",
    "                    batch_size=2,\n",
    "                    epochs=5,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "524288 / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(526000 - 524288) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "526 - 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Алгоритм\n",
    "# Берем последовательности ДНК 526.000. В нейронке (или в процессе формирования трен множества) клипаем их с двух сторон по 856 с каждой -> 524288\n",
    "# Берем Hi-C 526x526 -> клипаем по 7 c каждой стороны -> 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weights from the fudenberg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for layer in drosophila_model.layers[6:]:\n",
    "    if 'conv1d' in layer.name or 'batch_normalization' in layer.name or 'conv2d' in layer.name:\n",
    "        layer.set_weights(human_model_layers[count].get_weights())\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Data augmentation model\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 70/30 - train/test\n",
    "\n",
    "# train_indices = np.random.choice(1836, 1300, replace = False)\n",
    "# np.save(\"./train_test_data/v2/train_indices.npy\", train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FROM THE MEMORY\n",
    "train_indices = np.load(\"./train_test_data/v2/train_indices.npy\")\n",
    "test_indices = np.load(\"./train_test_data/v2/test_indices.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_indices))\n",
    "print(len(test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original X\n",
    "train_x = form_train_x(sequences, chroms, training_squares, train_indices)\n",
    "\n",
    "np.savez_compressed(\"./train_test_data/v2/train_x.npz\", train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse complement sequences\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "sequences_reverse_complemented = dict()\n",
    "\n",
    "for k,v in sequences_orig.items():\n",
    "    sequences_reverse_complemented[k] = str(Seq(v).reverse_complement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_dna(sequences_reverse_complemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse complemented\n",
    "train_x_reverse_complemented = form_train_x(sequences_reverse_complemented, chroms, training_squares, train_indices)\n",
    "\n",
    "np.savez_compressed(\"./train_test_data/v2/train_x_reverse_complemented.npz\", train_x_reverse_complemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Y\n",
    "train_y = form_train_y(training_squares, chroms, train_indices)\n",
    "\n",
    "np.savez_compressed(\"./train_test_data/v2/train_y.npz\", train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip training_squares along the diagonal\n",
    "\n",
    "import copy\n",
    "flipped_training_squares = copy.deepcopy(training_squares)\n",
    "\n",
    "for chrom in chroms:     \n",
    "    cur_training_squares = training_squares[chrom]\n",
    "\n",
    "    for i in range(len(cur_training_squares)):\n",
    "        # convert to list to allow modifications\n",
    "        flipped_training_squares[chrom][i] = list(flipped_training_squares[chrom][i])\n",
    "        \n",
    "        flipped_training_squares[chrom][i][0] = np.rot90(cur_training_squares[i][0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(training_squares['chrX'][152][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(flipped_training_squares['chrX'][152][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flipped along diagonal\n",
    "train_y_flipped = form_train_y(flipped_training_squares, chroms, train_indices)\n",
    "\n",
    "np.savez_compressed(\"./train_test_data/v2/train_y_flipped.npz\", train_y_flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Original and Reverse complement datasets into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig = np.load(\"./train_test_data/v2/train_x.npz\")['arr_0']\n",
    "train_x_reverse_complemented = np.load(\"./train_test_data/v2/train_x_reverse_complemented.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_combined = np.concatenate((train_x_orig, train_x_reverse_complemented), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600, 524288, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_orig = np.load(\"./train_test_data/v2/train_y.npz\")['arr_0']\n",
    "train_y_flipped = np.load(\"./train_test_data/v2/train_y_flipped.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_combined = np.concatenate((train_y_orig, train_y_flipped), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600, 130305, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle X and Y randomly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split big dataset into 3 smaller datsets (because of Google Colab RAM capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = form_train_x(sequences, chroms, training_squares, test_indices)\n",
    "test_y = form_train_y(training_squares, chroms, test_indices)\n",
    "\n",
    "np.savez_compressed(\"./test_x.npz\", test_x)\n",
    "np.savez_compressed(\"./test_y.npz\", test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
