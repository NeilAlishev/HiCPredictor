{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Notes\n",
    "\n",
    "в пределах 10мб - не мусорные контакты\n",
    "всю Hi-C карту предсказывать не надо. участки вдоль диагонали\n",
    "\n",
    "удалить главную диагональ? - схлопывать\n",
    "\n",
    "\n",
    "TODO: реализовать RMSE и R^2 для численной оценки качества предсказаний моделей\n",
    "Разделить на 80 training, 10 dev, 10 test\n",
    "Посмотреть качество предсказаний на test set'е\n",
    "\n",
    "\n",
    "TODO: \n",
    "Learn only on upper triangle (2 times less output values, should be faster). \n",
    "Then, after prediction, form 2d matrix from upper triangle\n",
    "\n",
    "TODO:\n",
    "Use all chromosomes (not only one) for training and testing NN\n",
    "\n",
    "\n",
    "TODO:\n",
    "To reduce overfitting and increase generalization\n",
    "stochastically shift input sequences by up to +/- 11 bp and reverse complement the DNA and flip the Hi-C map.\n",
    "(described in Fudenberg)\n",
    "\n",
    "\n",
    "Refactoring is needed!\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "pandas.set_option('display.max_columns', 500)\n",
    "pandas.set_option('display.max_rows', 500)\n",
    "\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import cooler\n",
    "import cooltools as ct\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "import pickle\n",
    "\n",
    "import scipy\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following directive activates inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# allow to allocate resources for model training\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import set_session\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# Training set formation\n",
    "WINDOW_SIZE = 512\n",
    "STRIDE = 512 // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTIL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hic(matrix, use_log_scale = False):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    if use_log_scale:\n",
    "        im = ax.matshow(np.log10(matrix), cmap='YlOrRd')\n",
    "        fig.colorbar(im)\n",
    "    else:\n",
    "        im = ax.matshow(matrix, cmap='YlOrRd')\n",
    "        fig.colorbar(im)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_dna(sequences = {}):\n",
    "    for k,v in sequences.items():\n",
    "        seq_array = np.array(list(v))\n",
    "\n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded_seq = label_encoder.fit_transform(seq_array)\n",
    "\n",
    "        integer_encoded_seq = integer_encoded_seq.reshape(len(integer_encoded_seq), 1)\n",
    "\n",
    "        onehot_encoder = OneHotEncoder(sparse = False)\n",
    "        result = onehot_encoder.fit_transform(integer_encoded_seq)\n",
    "        \n",
    "        # if Ns are present in the DNA sequence, result will have 5 columns. We delete 4th column which has Ns\n",
    "        # N row in the resulting training set will have all 0s\n",
    "        if result.shape[1] == 5:\n",
    "            result = np.delete(result, 3, 1)\n",
    "        \n",
    "        sequences[k] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_training_squares(hic_library, chroms):\n",
    "    training_set = {}\n",
    "\n",
    "    for chrom in chroms:\n",
    "        current_chrom = hic_library[chrom]\n",
    "        training_set[chrom] = []\n",
    "        \n",
    "        for part_number in sorted(current_chrom.keys()):\n",
    "            current_chrom_part = current_chrom[part_number]\n",
    "            \n",
    "            for i in range(WINDOW_SIZE, current_chrom_part.shape[0], STRIDE):\n",
    "                training_set[chrom].append((current_chrom_part[i - WINDOW_SIZE:i, i - WINDOW_SIZE:i],\n",
    "                                          (i - WINDOW_SIZE, i)))\n",
    "            \n",
    "    return training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_train_x(sequences_one_hot, chroms, training_squares):\n",
    "    train_x = []\n",
    "    \n",
    "    for chrom in chroms:\n",
    "        cur_seq = sequences_one_hot[chrom]\n",
    "        cur_training_squares = training_squares[chrom]\n",
    "\n",
    "        for training_square in cur_training_squares:\n",
    "            sq_begin, sq_end = training_square[1]\n",
    "            train_x.append(cur_seq[(sq_begin * 1000):(sq_end * 1000), ])\n",
    "\n",
    "            \n",
    "    return np.asarray(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_train_y(training_squares, chroms):\n",
    "    train_y = []\n",
    "    \n",
    "    for chrom in chroms:            \n",
    "        cur_training_squares = training_squares[chrom]\n",
    "\n",
    "        for training_square in cur_training_squares:\n",
    "            # TODO: TRY WITHOUT CROP\n",
    "            CROPPING_TARGET = 32\n",
    "            cropped_training_square = training_square[0][CROPPING_TARGET:training_square[0].shape[0] - CROPPING_TARGET, \n",
    "                                                         CROPPING_TARGET:training_square[0].shape[1] - CROPPING_TARGET]\n",
    "\n",
    "            upper_triu = to_upper_triu(cropped_training_square, diagonal_offset=2)\n",
    "            train_y.append(upper_triu)\n",
    "            \n",
    "            \n",
    "    return np.asarray(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_upper_triu(input_matrix, diagonal_offset = 2):\n",
    "    seq_len = input_matrix.shape[0]\n",
    "    return input_matrix[np.triu_indices(seq_len, diagonal_offset)].reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab specific code\n",
    "# filepath = \"drive/My Drive/Colab Notebooks/S2-Wang2017-async.dm3.mapq_30.100.mcool\"\n",
    "\n",
    "filepath = \"S2-Wang2017-async.dm3.mapq_30.100.mcool\"\n",
    "\n",
    "resolution = \"::/resolutions/1000\" # 1 KB resolution\n",
    "c = cooler.Cooler(filepath + resolution)\n",
    "\n",
    "chroms = c.chromnames\n",
    "# don't use these small chromosomes\n",
    "chroms.remove(\"chrM\")\n",
    "chroms.remove(\"chr4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T RUN: Transform all chromosome Hi-C maps and write them to file\n",
    "for chrom in chroms:\n",
    "    arr = c.matrix(balance=True).fetch(chrom)\n",
    "    # For adaptive coarse-grain transformation purposes\n",
    "    arr_raw = c.matrix(balance=False).fetch(chrom)\n",
    "    \n",
    "    transformed_arr = transform_hic(arr, arr_raw)\n",
    "    file_name = chrom + \"_transformed.npy\"\n",
    "    np.save(f\"./transformed_hic/{file_name}\", transformed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(arr, use_log_scale = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all transformed Hi-C\n",
    "transformed_hic = {}\n",
    "for chrom in chroms:\n",
    "    chrom_hic_filenames = [filename for filename in os.listdir('./transformed_hic') if filename.startswith(chrom)]\n",
    "    \n",
    "    if len(chrom_hic_filenames) > 0:\n",
    "        chrom_parts = {}\n",
    "        for filename in chrom_hic_filenames:\n",
    "            current_part_number = int(re.search(\"(transformed)(\\d+)\", filename).group(2))\n",
    "            chrom_parts[current_part_number] = np.load(f\"./transformed_hic/{filename}\")['arr_0']\n",
    "\n",
    "        transformed_hic[chrom] = chrom_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_squares = select_training_squares(transformed_hic, chroms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = {}\n",
    "for chrom in chroms:\n",
    "    fasta_sequence = list(SeqIO.parse(open(f\"./chromFa/{chrom}.fa\"),'fasta'))[0]\n",
    "    sequences[chrom] = str(fasta_sequence.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Воспринимаю маленькие и большие буквы одинаковым образом (??? Это норм ???)\n",
    "for k in sequences.keys():\n",
    "    sequences[k] = sequences[k].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразую последовательности в one-hot encoding (A = 0, C = 1, G = 2, T = 3)\n",
    "one_hot_dna(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = form_train_x(sequences, chroms, training_squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = form_train_y(training_squares, chroms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"./train_x.npz\", train_x)\n",
    "np.savez_compressed(\"./train_y.npz\", train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.load(\"./train_x.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convnet model (similar to Fudenberg NN):\n",
    "\n",
    "model_m = Sequential()\n",
    "\n",
    "model_m.add(layers.Conv1D(25, 50, activation='relu', input_shape=(50000, 4)))\n",
    "model_m.add(layers.Conv1D(25, 50, activation='relu'))\n",
    "model_m.add(layers.MaxPooling1D(5, strides = 2))\n",
    "\n",
    "model_m.add(layers.Conv1D(50, 25, activation='relu'))\n",
    "model_m.add(layers.MaxPooling1D(5, strides = 2))\n",
    "\n",
    "model_m.add(layers.Conv1D(50, 25, activation='relu'))\n",
    "model_m.add(layers.MaxPooling1D(20, strides = 4))\n",
    "\n",
    "model_m.add(layers.Conv1D(70, 20, activation='relu'))\n",
    "model_m.add(layers.MaxPooling1D(25, strides = 4))\n",
    "\n",
    "# dilated layers\n",
    "model_m.add(layers.Conv1D(100, 15, activation='relu', dilation_rate = 2))\n",
    "model_m.add(layers.Conv1D(100, 15, activation='relu', dilation_rate = 2))\n",
    "model_m.add(layers.MaxPooling1D(25, strides = 4))\n",
    "\n",
    "model_m.add(layers.Flatten())\n",
    "model_m.add(layers.Dense(2500, activation='linear'))\n",
    "\n",
    "# здесь можно не использовать входы (50 штук) и схлопывать после предсказания НС\n",
    "# не 2.500, а 2.450\n",
    "# для тренировки и для предсказаний надо будет сначала делать трансформацию для вектора\n",
    "\n",
    "print(model_m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
    "              loss='mse',\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_m.fit(train_x,\n",
    "                      train_y,\n",
    "                      batch_size=32,\n",
    "                      epochs=3,\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.load_weights('project2_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(training_squares[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_begin, sq_end = training_squares[0][1]\n",
    "seq_to_use = current_seq_one_hot[(sq_begin * 1000):(sq_end * 1000), ].reshape((1,50000, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model_m.predict(seq_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(prediction.reshape((50, 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.load_weights('project2_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(training_squares[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_begin, sq_end = training_squares[3][1]\n",
    "seq_to_use = current_seq_one_hot[(sq_begin * 1000):(sq_end * 1000), ].reshape((1,50000, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model_m.predict(seq_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(prediction.reshape((50, 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fudenberg избавляется от scaling'а\n",
    "# избавиться от шкалирования. полимерное свойство хроматина нам не обязательно выучивать\n",
    "# Нам надо это делать:\n",
    "# observed/expected - Саша отправит\n",
    "# можно применять observed/expected для всей хромосомы или для каждого квардрата 50x50\n",
    "# для каждого квардрата делать observed/expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Басет работа - 2015 (размеры слоев и параметры оттуда)\n",
    "# В басет предсказывается DNA sequence -> accessibility (open versus closed chromatin) of this area \n",
    "# (in different cell types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convnet model (similar to Basset NN):\n",
    "# Basset NN is not trainable in a reasonable time with this 50x50 window size -> reduced to 25x25 window size\n",
    "# Nevertheless, 3 times more tunable parameters than Fudenberg\n",
    "\n",
    "model_m = Sequential()\n",
    "\n",
    "model_m.add(layers.Conv1D(300, 21, activation='relu', input_shape=(25000, 4)))\n",
    "model_m.add(layers.MaxPooling1D(4))\n",
    "\n",
    "model_m.add(layers.Conv1D(300, 6, activation='relu'))\n",
    "model_m.add(layers.MaxPooling1D(4))\n",
    "\n",
    "model_m.add(layers.Conv1D(500, 4, activation='relu'))\n",
    "model_m.add(layers.MaxPooling1D(4))\n",
    "\n",
    "model_m.add(layers.Flatten())\n",
    "\n",
    "model_m.add(layers.Dense(1000, activation='relu'))\n",
    "\n",
    "model_m.add(layers.Dense(625, activation='linear'))\n",
    "\n",
    "print(model_m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.load_weights('project2_model_Basset.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(training_squares[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_begin, sq_end = training_squares[3][1]\n",
    "seq_to_use = current_seq_one_hot[(sq_begin * 1000):(sq_end * 1000), ].reshape((1,25000, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model_m.predict(seq_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(prediction.reshape((25, 25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hi-C tranformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(arr[0:1000, 0:1000], use_log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Adaptive coarse-grain\n",
    "transformed_arr = ct.lib.numutils.adaptive_coarsegrain(arr[0:1000, 0:1000], arr_raw[0:1000, 0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr, use_log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Normalize the contact matrix for distance-dependent contact decay.\n",
    "# Observed/Expected\n",
    "transformed_arr, _, _, _ = ct.lib.numutils.observed_over_expected(transformed_arr, mask = ~np.isnan(transformed_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr, use_log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Take natural logarithm\n",
    "transformed_arr = np.log(transformed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Interpolate all NaN values\n",
    "transformed_arr = ct.lib.numutils.interp_nan(transformed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Use Gaussian filter\n",
    "# To get rid of noise, emphasizing larger patterns.\n",
    "transformed_arr = scipy.ndimage.gaussian_filter(transformed_arr, sigma = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transformed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the transformations in one function\n",
    "def transform_hic(hic_matrix, hic_matrix_raw):\n",
    "    transformed_arr = ct.lib.numutils.adaptive_coarsegrain(hic_matrix, hic_matrix_raw)\n",
    "    transformed_arr, _, _, _ = ct.lib.numutils.observed_over_expected(transformed_arr, mask = ~np.isnan(transformed_arr))\n",
    "    transformed_arr = np.log(transformed_arr)\n",
    "    transformed_arr = ct.lib.numutils.interp_nan(transformed_arr)\n",
    "    transformed_arr = scipy.ndimage.gaussian_filter(transformed_arr, sigma = 1)\n",
    "    \n",
    "    return transformed_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(arr[0:4000, 0:4000], use_log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hic(transform_hic(arr[2500:3000, 2500:3000], arr_raw[2500:3000, 2500:3000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: RMSE and R^2. split on train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (None, 5, 3) -> (None, 5, 5, 3)\n",
    "# Explanation of the 1D -> 2D procedure\n",
    "\n",
    "oned = tf.constant([[[1, 6, 11], \n",
    "                     [2, 7, 12], \n",
    "                     [3, 8, 13],\n",
    "                     [4, 9, 14],\n",
    "                     [5, 10, 15]],\n",
    "                   \n",
    "                    [[1, 6, 11], \n",
    "                     [2, 7, 12], \n",
    "                     [3, 8, 13],\n",
    "                     [4, 9, 14],\n",
    "                     [5, 10, 15]],\n",
    "                   \n",
    "                    [[1, 6, 11], \n",
    "                     [2, 7, 12], \n",
    "                     [3, 8, 13],\n",
    "                     [4, 9, 14],\n",
    "                     [5, 10, 15]]])\n",
    "\n",
    "_, seq_len, features = oned.shape\n",
    "twod1 = tf.tile(oned, [1, seq_len, 1])\n",
    "twod1 = tf.reshape(twod1, [-1, seq_len, seq_len, features])\n",
    "twod2 = tf.transpose(twod1, [0,2,1,3])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Original tiled tensor for one 1D filter (1D filter expanded 5 times)\")\n",
    "    print(np.array(sess.run([twod1]))[0][0][:, :, 0])\n",
    "    \n",
    "    print(\"Transposed tiled tensor\")\n",
    "    print(np.array(sess.run([twod2]))[0][0][:, :, 0])\n",
    "\n",
    "\n",
    "twod1 = tf.expand_dims(twod1, axis=-1)\n",
    "twod2 = tf.expand_dims(twod2, axis=-1)\n",
    "twod  = tf.concat([twod1, twod2], axis=-1)\n",
    "twod = tf.reduce_mean(twod, axis=-1)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Result of mean operation between original and transposed (mean between each pair of values in 1D filter)\")\n",
    "    print(np.array(sess.run([twod]))[0][0][:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exlanation of ConcatDist2D procedure\n",
    "# This layer adds one more channel which contains pairwise distances for the matrices obtained on the previous layer\n",
    "# (this should enhance model performance)\n",
    "# (None, 5, 5, 3) -> (None, 5, 5, 4)\n",
    "\n",
    "# For one training example -> (1,5,5,3)\n",
    "inputs = tf.random.uniform(shape=[1,5,5,3])\n",
    "\n",
    "input_shape = tf.shape(inputs)\n",
    "batch_size, seq_len = input_shape[0], input_shape[1]\n",
    "\n",
    "## concat 2D distance ##\n",
    "pos = tf.expand_dims(tf.range(0, seq_len), axis=-1)\n",
    "matrix_repr1 = tf.tile(pos, [1,seq_len])\n",
    "matrix_repr2 = tf.transpose(matrix_repr1, [1,0])\n",
    "dist  = tf.math.abs( tf.math.subtract(matrix_repr1, matrix_repr2) )\n",
    "dist = tf.dtypes.cast(dist, tf.float32)\n",
    "dist = tf.expand_dims(dist, axis=-1)\n",
    "dist = tf.expand_dims(dist, axis=0)\n",
    "dist = tf.tile(dist, [batch_size, 1, 1, 1])\n",
    "\n",
    "res = tf.concat([inputs, dist], axis=-1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(np.array(sess.run([res]))[0, :, :, :, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Custom Keras layers\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneToTwo(tf.keras.layers.Layer):\n",
    "    ''' Transform 1d to 2d with i,j vectors operated on.'''\n",
    "    def __init__(self, operation='mean'):\n",
    "        super(OneToTwo, self).__init__()\n",
    "\n",
    "    def call(self, oned):\n",
    "        _, seq_len, features = oned.shape\n",
    "\n",
    "        twod1 = tf.tile(oned, [1, seq_len, 1])\n",
    "        twod1 = tf.reshape(twod1, [-1, seq_len, seq_len, features])\n",
    "        twod2 = tf.transpose(twod1, [0,2,1,3])\n",
    "\n",
    "        twod1 = tf.expand_dims(twod1, axis=-1)\n",
    "        twod2 = tf.expand_dims(twod2, axis=-1)\n",
    "        twod  = tf.concat([twod1, twod2], axis=-1)\n",
    "        twod = tf.reduce_mean(twod, axis=-1)\n",
    "\n",
    "        return twod\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config['operation'] = self.operation\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDist2D(tf.keras.layers.Layer):\n",
    "    ''' Concatenate the pairwise distance to 2d feature matrix.'''\n",
    "    def __init__(self):\n",
    "        super(ConcatDist2D, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, seq_len = input_shape[0], input_shape[1]\n",
    "\n",
    "        ## concat 2D distance ##\n",
    "        pos = tf.expand_dims(tf.range(0, seq_len), axis=-1)\n",
    "        matrix_repr1 = tf.tile(pos, [1, seq_len])\n",
    "        matrix_repr2 = tf.transpose(matrix_repr1, [1, 0])\n",
    "        dist = tf.math.abs(tf.math.subtract(matrix_repr1, matrix_repr2))\n",
    "        dist = tf.dtypes.cast(dist, tf.float32)\n",
    "        dist = tf.expand_dims(dist, axis=-1)\n",
    "        dist = tf.expand_dims(dist, axis=0)\n",
    "        dist = tf.tile(dist, [batch_size, 1, 1, 1])\n",
    "        return tf.concat([inputs, dist], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Symmetrize2D(tf.keras.layers.Layer):\n",
    "    '''Take the average of a matrix and its transpose to enforce symmetry.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Symmetrize2D, self).__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        x_t = tf.transpose(x, [0, 2, 1, 3])\n",
    "        x_sym = (x + x_t) / 2\n",
    "        return x_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpperTri(tf.keras.layers.Layer):\n",
    "    ''' Unroll matrix to its upper triangular portion.'''\n",
    "\n",
    "    def __init__(self, diagonal_offset=2):\n",
    "        super(UpperTri, self).__init__()\n",
    "        self.diagonal_offset = diagonal_offset\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_len = inputs.shape[1].value\n",
    "        output_dim = inputs.shape[-1]\n",
    "\n",
    "        triu_tup = np.triu_indices(seq_len, self.diagonal_offset)\n",
    "        triu_index = list(triu_tup[0] + seq_len * triu_tup[1])\n",
    "        unroll_repr = tf.reshape(inputs, [-1, seq_len ** 2, output_dim])\n",
    "        return tf.gather(unroll_repr, triu_index, axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config['diagonal_offset'] = self.diagonal_offset\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticReverseComplement(tf.keras.layers.Layer):\n",
    "    \"\"\"Stochastically reverse complement a one hot encoded DNA sequence.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(StochasticReverseComplement, self).__init__()\n",
    "\n",
    "    def call(self, seq_1hot, training=None):\n",
    "        if training:\n",
    "            rc_seq_1hot = tf.gather(seq_1hot, [3, 2, 1, 0], axis=-1)\n",
    "            rc_seq_1hot = tf.reverse(rc_seq_1hot, axis=[1])\n",
    "            reverse_bool = tf.random.uniform(shape=[]) > 0.5\n",
    "            src_seq_1hot = tf.cond(reverse_bool, lambda: rc_seq_1hot, lambda: seq_1hot)\n",
    "            return src_seq_1hot, reverse_bool\n",
    "        else:\n",
    "            return seq_1hot, tf.constant(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticShift(tf.keras.layers.Layer):\n",
    "    \"\"\"Stochastically shift a one hot encoded DNA sequence.\"\"\"\n",
    "\n",
    "    def __init__(self, shift_max=0, pad='uniform'):\n",
    "        super(StochasticShift, self).__init__()\n",
    "        self.shift_max = shift_max\n",
    "        self.augment_shifts = tf.range(-self.shift_max, self.shift_max + 1)\n",
    "        self.pad = pad\n",
    "\n",
    "    def call(self, seq_1hot, training=None):\n",
    "        if training:\n",
    "            shift_i = tf.random.uniform(shape=[], minval=0, dtype=tf.int64,\n",
    "                                        maxval=len(self.augment_shifts))\n",
    "            shift = tf.gather(self.augment_shifts, shift_i)\n",
    "            sseq_1hot = tf.cond(tf.not_equal(shift, 0),\n",
    "                                lambda: shift_sequence(seq_1hot, shift),\n",
    "                                lambda: seq_1hot)\n",
    "            return sseq_1hot\n",
    "        else:\n",
    "            return seq_1hot\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'shift_max': self.shift_max,\n",
    "            'pad': self.pad\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "def shift_sequence(seq, shift, pad_value=0.25):\n",
    "    \"\"\"Shift a sequence left or right by shift_amount.\n",
    "    Args:\n",
    "    seq: [batch_size, seq_length, seq_depth] sequence\n",
    "    shift: signed shift value (tf.int32 or int)\n",
    "    pad_value: value to fill the padding (primitive or scalar tf.Tensor)\n",
    "    \"\"\"\n",
    "    if seq.shape.ndims != 3:\n",
    "        raise ValueError('input sequence should be rank 3')\n",
    "    input_shape = seq.shape\n",
    "\n",
    "    pad = pad_value * tf.ones_like(seq[:, 0:tf.abs(shift), :])\n",
    "\n",
    "    def _shift_right(_seq):\n",
    "        # shift is positive\n",
    "        sliced_seq = _seq[:, :-shift:, :]\n",
    "        return tf.concat([pad, sliced_seq], axis=1)\n",
    "\n",
    "    def _shift_left(_seq):\n",
    "        # shift is negative\n",
    "        sliced_seq = _seq[:, -shift:, :]\n",
    "        return tf.concat([sliced_seq, pad], axis=1)\n",
    "\n",
    "    sseq = tf.cond(tf.greater(shift, 0),\n",
    "                   lambda: _shift_right(seq),\n",
    "                   lambda: _shift_left(seq))\n",
    "    sseq.set_shape(input_shape)\n",
    "\n",
    "    return sseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchReverse(tf.keras.layers.Layer):\n",
    "    \"\"\"Reverse predictions if the inputs were reverse complemented.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SwitchReverse, self).__init__()\n",
    "\n",
    "    def call(self, x_reverse):\n",
    "        x = x_reverse[0]\n",
    "        reverse = x_reverse[1]\n",
    "\n",
    "        xd = len(x.shape)\n",
    "        if xd == 3:\n",
    "            rev_axes = [1]\n",
    "        elif xd == 4:\n",
    "            rev_axes = [1, 2]\n",
    "        else:\n",
    "            raise ValueError('Cannot recognize SwitchReverse input dimensions %d.' % xd)\n",
    "\n",
    "        return tf.keras.backend.switch(reverse,\n",
    "                                       tf.reverse(x, axis=rev_axes),\n",
    "                                       x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Helper functions\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(current, activation, verbose=False):\n",
    "    if verbose: \n",
    "        print('activate:',activation)\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        current = tf.keras.layers.ReLU()(current)\n",
    "    elif activation == 'gelu':\n",
    "        current = GELU()(current)\n",
    "    elif activation == 'sigmoid':\n",
    "        current = tf.keras.layers.Activation('sigmoid')(current)\n",
    "    elif activation == 'tanh':\n",
    "        current = tf.keras.layers.Activation('tanh')(current)\n",
    "    elif activation == 'exp':\n",
    "        current = Exp()(current)\n",
    "    elif activation == 'softplus':\n",
    "        current = Softplus()(current)\n",
    "    else:\n",
    "        print('Unrecognized activation \"%s\"' % activation, file=sys.stderr)\n",
    "        exit(1)\n",
    "\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Keras blocks\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inputs, filters=None, kernel_size=1, activation='relu', strides=1,\n",
    "    dilation_rate=1, l2_scale=0, dropout=0, conv_type='standard', residual=False,\n",
    "    pool_size=1, batch_norm=False, bn_momentum=0.99, bn_gamma=None,\n",
    "    kernel_initializer='he_normal'):\n",
    "  \n",
    "    \"\"\"Construct a single convolution block.\n",
    "    Args:\n",
    "    inputs:        [batch_size, seq_length, features] input sequence\n",
    "    filters:       Conv1D filters\n",
    "    kernel_size:   Conv1D kernel_size\n",
    "    activation:    relu/gelu/etc\n",
    "    strides:       Conv1D strides\n",
    "    dilation_rate: Conv1D dilation rate\n",
    "    l2_scale:      L2 regularization weight.\n",
    "    dropout:       Dropout rate probability\n",
    "    conv_type:     Conv1D layer type\n",
    "    residual:      Residual connection boolean\n",
    "    pool_size:     Max pool width\n",
    "    batch_norm:    Apply batch normalization\n",
    "    bn_momentum:   BatchNorm momentum\n",
    "    bn_gamma:      BatchNorm gamma (defaults according to residual)\n",
    "    Returns:\n",
    "    [batch_size, seq_length, features] output sequence\n",
    "    \"\"\"\n",
    "\n",
    "    # flow through variable current\n",
    "    current = inputs\n",
    "\n",
    "    # choose convolution type\n",
    "    if conv_type == 'separable':\n",
    "        conv_layer = tf.keras.layers.SeparableConv1D\n",
    "    else:\n",
    "        conv_layer = tf.keras.layers.Conv1D\n",
    "\n",
    "    if filters is None:\n",
    "        filters = inputs.shape[-1]\n",
    "\n",
    "    # activation\n",
    "    current = activate(current, activation)\n",
    "\n",
    "    # convolution\n",
    "    current = conv_layer(\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        use_bias=False,\n",
    "        dilation_rate=dilation_rate,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(l2_scale))(current)\n",
    "\n",
    "    # batch norm\n",
    "    if batch_norm:\n",
    "        if bn_gamma is None:\n",
    "            bn_gamma = 'zeros' if residual else 'ones'\n",
    "        \n",
    "        current = tf.keras.layers.BatchNormalization(momentum=bn_momentum, gamma_initializer=bn_gamma, fused=True)(current)\n",
    "\n",
    "    # dropout\n",
    "    if dropout > 0:\n",
    "        current = tf.keras.layers.Dropout(rate=dropout)(current)\n",
    "\n",
    "    # residual add\n",
    "    if residual:\n",
    "        current = tf.keras.layers.Add()([inputs,current])\n",
    "\n",
    "    # Pool\n",
    "    if pool_size > 1:\n",
    "        current = tf.keras.layers.MaxPool1D(pool_size=pool_size, padding='same')(current)\n",
    "\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_tower(inputs, filters_init, filters_mult=1, repeat=1, **kwargs):\n",
    "    \"\"\"Construct a reducing convolution block.\n",
    "    Args:\n",
    "    inputs:        [batch_size, seq_length, features] input sequence\n",
    "    filters_init:  Initial Conv1D filters\n",
    "    filters_mult:  Multiplier for Conv1D filters\n",
    "    repeat:        Conv block repetitions\n",
    "    Returns:\n",
    "    [batch_size, seq_length, features] output sequence\n",
    "    \"\"\"\n",
    "\n",
    "    # flow through variable current\n",
    "    current = inputs\n",
    "\n",
    "    # initialize filters\n",
    "    rep_filters = filters_init\n",
    "\n",
    "    for ri in range(repeat):\n",
    "        # convolution\n",
    "        current = conv_block(current, filters=int(np.round(rep_filters)), **kwargs)\n",
    "\n",
    "    # update filters\n",
    "    rep_filters *= filters_mult\n",
    "\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilated_residual(inputs, filters, kernel_size=3, rate_mult=2, conv_type='standard', \n",
    "                     dropout=0, repeat=1, round=False, **kwargs):\n",
    "    \"\"\"Construct a residual dilated convolution block.\n",
    "    Args:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "\n",
    "    # flow through variable current\n",
    "    current = inputs\n",
    "\n",
    "    # initialize dilation rate\n",
    "    dilation_rate = 1.0\n",
    "\n",
    "    for ri in range(repeat):\n",
    "        # For skip connection purpose\n",
    "        rep_input = current\n",
    "\n",
    "        # dilate\n",
    "        current = conv_block(current, filters=filters, kernel_size=kernel_size, \n",
    "                             dilation_rate=int(np.round(dilation_rate)), \n",
    "                             conv_type=conv_type, bn_gamma='ones', **kwargs)\n",
    "\n",
    "        # return\n",
    "        current = conv_block(current, filters=int(rep_input.shape[-1]), dropout=dropout, bn_gamma='zeros', **kwargs)\n",
    "\n",
    "        # residual add\n",
    "        current = tf.keras.layers.Add()([rep_input, current])\n",
    "\n",
    "        # update dilation rate\n",
    "        dilation_rate *= rate_mult\n",
    "        \n",
    "        if round:\n",
    "            dilation_rate = np.round(dilation_rate)\n",
    "\n",
    "    return current\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D related blocks\n",
    "\n",
    "def concat_dist_2d(inputs, **kwargs):\n",
    "    current = ConcatDist2D()(inputs)\n",
    "    return current\n",
    "\n",
    "def one_to_two(inputs, operation='mean', **kwargs):\n",
    "    current = OneToTwo(operation)(inputs)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block_2d(inputs, filters=128, activation='relu', conv_type='standard',\n",
    "    kernel_size=1, strides=1, dilation_rate=1, l2_scale=0, dropout=0, pool_size=1,\n",
    "    batch_norm=False, bn_momentum=0.99, bn_gamma='ones', symmetric=False):\n",
    "\n",
    "    \"\"\"Construct a single 2D convolution block.   \"\"\"\n",
    "\n",
    "    # flow through variable current\n",
    "    current = inputs\n",
    "\n",
    "    # activation\n",
    "    current = activate(current, activation)\n",
    "\n",
    "  # choose convolution type\n",
    "    if conv_type == 'separable':\n",
    "        conv_layer = tf.keras.layers.SeparableConv2D\n",
    "    else:\n",
    "        conv_layer = tf.keras.layers.Conv2D\n",
    "\n",
    "    # convolution\n",
    "    current = conv_layer(\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        use_bias=False,\n",
    "        dilation_rate=dilation_rate,\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(l2_scale))(current)\n",
    "\n",
    "    # batch norm\n",
    "    if batch_norm:\n",
    "        current = tf.keras.layers.BatchNormalization(\n",
    "          momentum=bn_momentum,\n",
    "          gamma_initializer=bn_gamma,\n",
    "          fused=True)(current)\n",
    "\n",
    "    # dropout\n",
    "    if dropout > 0:\n",
    "        current = tf.keras.layers.Dropout(rate=dropout)(current)\n",
    "\n",
    "    # pool\n",
    "    if pool_size > 1:\n",
    "        current = tf.keras.layers.MaxPool2D(\n",
    "          pool_size=pool_size,\n",
    "          padding='same')(current)\n",
    "\n",
    "    # symmetric\n",
    "    if symmetric:\n",
    "        current = layers.Symmetrize2D()(current)\n",
    "\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetrize_2d(inputs, **kwargs):\n",
    "    return Symmetrize2D()(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilated_residual_2d(inputs, filters, kernel_size=3, rate_mult=2,\n",
    "                        dropout=0, repeat=1, symmetric=True, **kwargs):\n",
    "    \"\"\"Construct a residual dilated convolution block.\n",
    "    \"\"\"\n",
    "\n",
    "    # flow through variable current\n",
    "    current = inputs\n",
    "\n",
    "    # initialize dilation rate\n",
    "    dilation_rate = 1.0\n",
    "\n",
    "    for ri in range(repeat):\n",
    "        rep_input = current\n",
    "\n",
    "        # dilate\n",
    "        current = conv_block_2d(current,\n",
    "                                filters=filters,\n",
    "                                kernel_size=kernel_size,\n",
    "                                dilation_rate=int(np.round(dilation_rate)),\n",
    "                                bn_gamma='ones',\n",
    "                                **kwargs)\n",
    "\n",
    "        # return\n",
    "        current = conv_block_2d(current,\n",
    "                                filters=int(rep_input.shape[-1]),\n",
    "                                dropout=dropout,\n",
    "                                bn_gamma='zeros',\n",
    "                                **kwargs)\n",
    "\n",
    "        # residual add\n",
    "        current = tf.keras.layers.Add()([rep_input, current])\n",
    "\n",
    "        # enforce symmetry\n",
    "        if symmetric:\n",
    "            current = Symmetrize2D()(current)\n",
    "\n",
    "        # update dilation rate\n",
    "        dilation_rate *= rate_mult\n",
    "\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropping_2d(inputs, cropping, **kwargs):\n",
    "    current = tf.keras.layers.Cropping2D(cropping)(inputs)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_tri(inputs, diagonal_offset=2, **kwargs):\n",
    "    current = UpperTri(diagonal_offset)(inputs)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(inputs, units, activation='softplus', kernel_initializer='he_normal',\n",
    "          l2_scale=0, l1_scale=0, **kwargs):\n",
    "    \n",
    "    current = tf.keras.layers.Dense(\n",
    "        units=units,\n",
    "        activation=activation,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        kernel_regularizer=tf.keras.regularizers.l1_l2(l1_scale, l2_scale)\n",
    "    )(inputs)\n",
    "    \n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "\n",
    "# Changed for SEQ_LENGTH = 512000\n",
    "\n",
    "#SEQ_LENGTH = 512000\n",
    "SEQ_LENGTH = 1048576\n",
    "sequence = tf.keras.Input(shape=(SEQ_LENGTH, 4), name='sequence')\n",
    "\n",
    "current = sequence\n",
    "\n",
    "# Augmentation - Enable it later (after good performance on the training set)\n",
    "# current, reverse_bool = StochasticReverseComplement()(current)\n",
    "# augment_shift = 11\n",
    "# current = StochasticShift(augment_shift)(current)\n",
    "\n",
    "# TRUNK:\n",
    "\n",
    "# First 1D convolution\n",
    "current = conv_block(current, filters=96, kernel_size=11, pool_size=2, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "\n",
    "# Change for repeat = 9\n",
    "\n",
    "# Multiple (11) 1D convolutions in a tower to arrive to 2048bp representation in 1D vectors\n",
    "current = conv_tower(current, filters_init=96, filters_mult=1.0, kernel_size=5, pool_size=2, repeat=9, \n",
    "                     batch_norm=True, bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# Dilated residual layers\n",
    "current = dilated_residual(current, filters=48, rate_mult=1.75, repeat=8, dropout=0.4, batch_norm=True, \n",
    "                           bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# Bottleneck 1D convolution\n",
    "current = conv_block(current, filters=64, kernel_size=5, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "# final activation\n",
    "current = activate(current, \"relu\")\n",
    "\n",
    "\n",
    "# HEAD:\n",
    "current = one_to_two(current)\n",
    "current = concat_dist_2d(current)\n",
    "current = conv_block_2d(current, filters=48, kernel_size=3, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "current = symmetrize_2d(current)\n",
    "current = dilated_residual_2d(current, filters=24, kernel_size=3, rate_mult=1.75, repeat=6, dropout=0.1,\n",
    "                              batch_norm=True, bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# TODO: TRY WITHOUT CROP\n",
    "current = cropping_2d(current, cropping=26)\n",
    "\n",
    "current = upper_tri(current, diagonal_offset=2)\n",
    "\n",
    "#current = dense(current, units=1, activation=\"linear\")\n",
    "current = dense(current, units=5, activation=\"linear\")\n",
    "\n",
    "# current = SwitchReverse()([current, reverse_bool])\n",
    "\n",
    "# make model\n",
    "model = tf.keras.Model(inputs=sequence, outputs=current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_sgd = tf.keras.optimizers.SGD(\n",
    "          lr=0.0065,\n",
    "          momentum=0.99575,\n",
    "          clipnorm=10.7)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.MSE,\n",
    "                    optimizer=optimizer_sgd,\n",
    "                    metrics=['mae']) # TODO: R^2 and Person custom metrics (? TensorBoard and EarlyStopping ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_x,\n",
    "                    train_y,\n",
    "                    batch_size=2,\n",
    "                    epochs=5,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, we implement NN without stochastic shift and hi-c matrix flip\n",
    "# this will help performance and is TODO\n",
    "# Also Fudenberg trains on 5 datasets at the same time (Multi-task training). This improves accuracy on all dataset predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убрать crop, уменьшить НС, multitask learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Fine-tuning method\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "# у его one-hot-encoding - ACGT Порядок сделать у меня такой же (поменять training set)\n",
    "# change training set to 512x512\n",
    "\n",
    "\n",
    "import json\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1' ### run on CPU\n",
    "print(tf.__version__)\n",
    "if tf.__version__[0] == '1':\n",
    "    tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "from cooltools.lib.numutils import set_diag\n",
    "from basenji import dataset, dna_io, seqnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           [(None, 1048576, 4)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "stochastic_reverse_complement ( ((None, 1048576, 4), 0           sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "stochastic_shift (StochasticShi (None, 1048576, 4)   0           stochastic_reverse_complement[0][\n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 1048576, 4)   0           stochastic_shift[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 1048576, 96)  4224        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1048576, 96)  384         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 524288, 96)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 524288, 96)   0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 524288, 96)   46080       re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 524288, 96)   384         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 262144, 96)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 262144, 96)   0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 262144, 96)   46080       re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 262144, 96)   384         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 131072, 96)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 131072, 96)   0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 131072, 96)   46080       re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 131072, 96)   384         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 65536, 96)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 65536, 96)    0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 65536, 96)    46080       re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 65536, 96)    384         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 32768, 96)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 32768, 96)    0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 32768, 96)    46080       re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32768, 96)    384         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 16384, 96)    0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 16384, 96)    0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 16384, 96)    46080       re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16384, 96)    384         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 8192, 96)     0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 8192, 96)     0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 8192, 96)     46080       re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 8192, 96)     384         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 4096, 96)     0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 4096, 96)     0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 4096, 96)     46080       re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 4096, 96)     384         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 2048, 96)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 2048, 96)     0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 2048, 96)     46080       re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 2048, 96)     384         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 1024, 96)     0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, 1024, 96)     0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 1024, 96)     46080       re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 1024, 96)     384         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 512, 96)      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, 512, 96)      0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 512, 48)      13824       re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 512, 48)      192         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, 512, 48)      0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 512, 96)      4608        re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 512, 96)      384         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512, 96)      0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 512, 96)      0           max_pooling1d_10[0][0]           \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_13 (ReLU)                 (None, 512, 96)      0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, None, 48)     13824       re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, 48)     192         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, None, 48)     0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, None, 96)     4608        re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, 96)     384         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 96)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 512, 96)      0           add[0][0]                        \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_15 (ReLU)                 (None, 512, 96)      0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, None, 48)     13824       re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, 48)     192         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_16 (ReLU)                 (None, None, 48)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, None, 96)     4608        re_lu_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, None, 96)     384         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 96)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 512, 96)      0           add_1[0][0]                      \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_17 (ReLU)                 (None, 512, 96)      0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, None, 48)     13824       re_lu_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, None, 48)     192         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_18 (ReLU)                 (None, None, 48)     0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, None, 96)     4608        re_lu_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, None, 96)     384         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, None, 96)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 512, 96)      0           add_2[0][0]                      \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_19 (ReLU)                 (None, 512, 96)      0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, None, 48)     13824       re_lu_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, None, 48)     192         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_20 (ReLU)                 (None, None, 48)     0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, None, 96)     4608        re_lu_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, None, 96)     384         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, None, 96)     0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 512, 96)      0           add_3[0][0]                      \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_21 (ReLU)                 (None, 512, 96)      0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, None, 48)     13824       re_lu_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, None, 48)     192         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_22 (ReLU)                 (None, None, 48)     0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, None, 96)     4608        re_lu_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, None, 96)     384         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, None, 96)     0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 512, 96)      0           add_4[0][0]                      \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_23 (ReLU)                 (None, 512, 96)      0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, None, 48)     13824       re_lu_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, None, 48)     192         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_24 (ReLU)                 (None, None, 48)     0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, None, 96)     4608        re_lu_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, None, 96)     384         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, None, 96)     0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 512, 96)      0           add_5[0][0]                      \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_25 (ReLU)                 (None, 512, 96)      0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, None, 48)     13824       re_lu_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, None, 48)     192         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_26 (ReLU)                 (None, None, 48)     0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, None, 96)     4608        re_lu_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, None, 96)     384         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, None, 96)     0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 512, 96)      0           add_6[0][0]                      \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_27 (ReLU)                 (None, 512, 96)      0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 512, 64)      30720       re_lu_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 512, 64)      256         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_28 (ReLU)                 (None, 512, 64)      0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_to2d (AverageTo2D)      (None, 512, 512, 64) 0           re_lu_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concat_dist2d (ConcatDist2D)    (None, 512, 512, 65) 0           average_to2d[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_29 (ReLU)                 (None, 512, 512, 65) 0           concat_dist2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 512, 512, 48) 28080       re_lu_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 512, 512, 48) 192         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d (Symmetrize2D)     (None, 512, 512, 48) 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_30 (ReLU)                 (None, 512, 512, 48) 0           symmetrize2d[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 512, 512, 24) 10368       re_lu_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 512, 512, 24) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_31 (ReLU)                 (None, 512, 512, 24) 0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 512, 512, 48) 1152        re_lu_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 512, 512, 48) 192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 512, 512, 48) 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 512, 512, 48) 0           symmetrize2d[0][0]               \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_1 (Symmetrize2D)   (None, 512, 512, 48) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_32 (ReLU)                 (None, 512, 512, 48) 0           symmetrize2d_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 2 10368       re_lu_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, None, None, 2 96          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_33 (ReLU)                 (None, None, None, 2 0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 4 1152        re_lu_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, None, None, 4 192         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, None, None, 4 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 512, 512, 48) 0           symmetrize2d_1[0][0]             \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_2 (Symmetrize2D)   (None, 512, 512, 48) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_34 (ReLU)                 (None, 512, 512, 48) 0           symmetrize2d_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 2 10368       re_lu_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, None, None, 2 96          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_35 (ReLU)                 (None, None, None, 2 0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 4 1152        re_lu_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, None, None, 4 192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, None, None, 4 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 512, 512, 48) 0           symmetrize2d_2[0][0]             \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_3 (Symmetrize2D)   (None, 512, 512, 48) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_36 (ReLU)                 (None, 512, 512, 48) 0           symmetrize2d_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 2 10368       re_lu_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, None, None, 2 96          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_37 (ReLU)                 (None, None, None, 2 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 4 1152        re_lu_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, None, None, 4 192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, None, None, 4 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 512, 512, 48) 0           symmetrize2d_3[0][0]             \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_4 (Symmetrize2D)   (None, 512, 512, 48) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_38 (ReLU)                 (None, 512, 512, 48) 0           symmetrize2d_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 2 10368       re_lu_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, None, None, 2 96          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_39 (ReLU)                 (None, None, None, 2 0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 4 1152        re_lu_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, None, None, 4 192         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, None, None, 4 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 512, 512, 48) 0           symmetrize2d_4[0][0]             \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_5 (Symmetrize2D)   (None, 512, 512, 48) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_40 (ReLU)                 (None, 512, 512, 48) 0           symmetrize2d_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 2 10368       re_lu_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, None, None, 2 96          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_41 (ReLU)                 (None, None, None, 2 0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 4 1152        re_lu_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, None, None, 4 192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, None, None, 4 0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 512, 512, 48) 0           symmetrize2d_5[0][0]             \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_6 (Symmetrize2D)   (None, 512, 512, 48) 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d (Cropping2D)         (None, 448, 448, 48) 0           symmetrize2d_6[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "upper_tri (UpperTri)            (None, 99681, 48)    0           cropping2d[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 99681, 5)     245         upper_tri[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "switch_reverse_triu (SwitchReve (None, 99681, 5)     0           dense[0][0]                      \n",
      "                                                                 stochastic_reverse_complement[0][\n",
      "==================================================================================================\n",
      "Total params: 751,653\n",
      "Trainable params: 746,149\n",
      "Non-trainable params: 5,504\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [2048]\n",
      "target_lengths [99681]\n",
      "target_crops [-49585]\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"./basenji/\"\n",
    "params_file = model_dir+'params.json'\n",
    "model_file  = model_dir+'model_best.h5'\n",
    "with open(params_file) as params_open:\n",
    "    params = json.load(params_open)\n",
    "    params_model = params['model']\n",
    "    params_train = params['train']\n",
    "\n",
    "seq_length = params_model['seq_length']\n",
    "target_length = params_model['target_length']\n",
    "target_crop = params_model['target_crop']\n",
    "human_model = seqnn.SeqNN(params_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded\n"
     ]
    }
   ],
   "source": [
    "human_model.restore(model_file)\n",
    "print('successfully loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_model = human_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           [(None, 1048576, 4)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "stochastic_reverse_complement ( ((None, 1048576, 4), 0           sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "stochastic_shift (StochasticShi (None, 1048576, 4)   0           stochastic_reverse_complement[0][\n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 1048576, 4)   0           stochastic_shift[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 1048576, 96)  4224        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1048576, 96)  384         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 524288, 96)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 524288, 96)   0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 524288, 96)   46080       re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 524288, 96)   384         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 262144, 96)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 262144, 96)   0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 262144, 96)   46080       re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 262144, 96)   384         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 131072, 96)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 131072, 96)   0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 131072, 96)   46080       re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 131072, 96)   384         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 65536, 96)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 65536, 96)    0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 65536, 96)    46080       re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 65536, 96)    384         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 32768, 96)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 32768, 96)    0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 32768, 96)    46080       re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32768, 96)    384         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 16384, 96)    0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 16384, 96)    0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 16384, 96)    46080       re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16384, 96)    384         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 8192, 96)     0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 8192, 96)     0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 8192, 96)     46080       re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 8192, 96)     384         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 4096, 96)     0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 4096, 96)     0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 4096, 96)     46080       re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 4096, 96)     384         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 2048, 96)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 2048, 96)     0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 2048, 96)     46080       re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 2048, 96)     384         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 1024, 96)     0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, 1024, 96)     0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 1024, 96)     46080       re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 1024, 96)     384         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 512, 96)      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, 512, 96)      0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 512, 48)      13824       re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 512, 48)      192         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, 512, 48)      0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 512, 96)      4608        re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 512, 96)      384         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512, 96)      0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 512, 96)      0           max_pooling1d_10[0][0]           \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_13 (ReLU)                 (None, 512, 96)      0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, None, 48)     13824       re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, 48)     192         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, None, 48)     0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, None, 96)     4608        re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, 96)     384         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 96)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 512, 96)      0           add[0][0]                        \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_15 (ReLU)                 (None, 512, 96)      0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, None, 48)     13824       re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, 48)     192         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_16 (ReLU)                 (None, None, 48)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, None, 96)     4608        re_lu_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, None, 96)     384         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 96)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 512, 96)      0           add_1[0][0]                      \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_17 (ReLU)                 (None, 512, 96)      0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, None, 48)     13824       re_lu_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, None, 48)     192         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_18 (ReLU)                 (None, None, 48)     0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, None, 96)     4608        re_lu_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, None, 96)     384         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, None, 96)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 512, 96)      0           add_2[0][0]                      \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_19 (ReLU)                 (None, 512, 96)      0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, None, 48)     13824       re_lu_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, None, 48)     192         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_20 (ReLU)                 (None, None, 48)     0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, None, 96)     4608        re_lu_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, None, 96)     384         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, None, 96)     0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 512, 96)      0           add_3[0][0]                      \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_21 (ReLU)                 (None, 512, 96)      0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, None, 48)     13824       re_lu_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, None, 48)     192         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_22 (ReLU)                 (None, None, 48)     0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, None, 96)     4608        re_lu_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, None, 96)     384         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, None, 96)     0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 512, 96)      0           add_4[0][0]                      \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_23 (ReLU)                 (None, 512, 96)      0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, None, 48)     13824       re_lu_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, None, 48)     192         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_24 (ReLU)                 (None, None, 48)     0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, None, 96)     4608        re_lu_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, None, 96)     384         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, None, 96)     0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 512, 96)      0           add_5[0][0]                      \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_25 (ReLU)                 (None, 512, 96)      0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, None, 48)     13824       re_lu_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, None, 48)     192         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_26 (ReLU)                 (None, None, 48)     0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, None, 96)     4608        re_lu_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, None, 96)     384         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, None, 96)     0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 512, 96)      0           add_6[0][0]                      \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_27 (ReLU)                 (None, 512, 96)      0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 512, 64)      30720       re_lu_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 512, 64)      256         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_28 (ReLU)                 (None, 512, 64)      0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_to2d (AverageTo2D)      (None, 512, 512, 64) 0           re_lu_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concat_dist2d (ConcatDist2D)    (None, 512, 512, 65) 0           average_to2d[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_29 (ReLU)                 (None, 512, 512, 65) 0           concat_dist2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 512, 512, 48) 28080       re_lu_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 512, 512, 48) 192         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d (Symmetrize2D)     (None, 512, 512, 48) 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_30 (ReLU)                 (None, 512, 512, 48) 0           symmetrize2d[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 512, 512, 24) 10368       re_lu_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 512, 512, 24) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_31 (ReLU)                 (None, 512, 512, 24) 0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 512, 512, 48) 1152        re_lu_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 512, 512, 48) 192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 512, 512, 48) 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 512, 512, 48) 0           symmetrize2d[0][0]               \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_1 (Symmetrize2D)   (None, 512, 512, 48) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_32 (ReLU)                 (None, 512, 512, 48) 0           symmetrize2d_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 2 10368       re_lu_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, None, None, 2 96          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_33 (ReLU)                 (None, None, None, 2 0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 4 1152        re_lu_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, None, None, 4 192         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, None, None, 4 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 512, 512, 48) 0           symmetrize2d_1[0][0]             \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_2 (Symmetrize2D)   (None, 512, 512, 48) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_34 (ReLU)                 (None, 512, 512, 48) 0           symmetrize2d_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 2 10368       re_lu_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, None, None, 2 96          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_35 (ReLU)                 (None, None, None, 2 0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 4 1152        re_lu_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, None, None, 4 192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, None, None, 4 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 512, 512, 48) 0           symmetrize2d_2[0][0]             \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_3 (Symmetrize2D)   (None, 512, 512, 48) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_36 (ReLU)                 (None, 512, 512, 48) 0           symmetrize2d_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 2 10368       re_lu_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, None, None, 2 96          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_37 (ReLU)                 (None, None, None, 2 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 4 1152        re_lu_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, None, None, 4 192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, None, None, 4 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 512, 512, 48) 0           symmetrize2d_3[0][0]             \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_4 (Symmetrize2D)   (None, 512, 512, 48) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_38 (ReLU)                 (None, 512, 512, 48) 0           symmetrize2d_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 2 10368       re_lu_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, None, None, 2 96          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_39 (ReLU)                 (None, None, None, 2 0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 4 1152        re_lu_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, None, None, 4 192         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, None, None, 4 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 512, 512, 48) 0           symmetrize2d_4[0][0]             \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_5 (Symmetrize2D)   (None, 512, 512, 48) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_40 (ReLU)                 (None, 512, 512, 48) 0           symmetrize2d_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 2 10368       re_lu_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, None, None, 2 96          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_41 (ReLU)                 (None, None, None, 2 0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 4 1152        re_lu_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, None, None, 4 192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, None, None, 4 0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 512, 512, 48) 0           symmetrize2d_5[0][0]             \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_6 (Symmetrize2D)   (None, 512, 512, 48) 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d (Cropping2D)         (None, 448, 448, 48) 0           symmetrize2d_6[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "upper_tri (UpperTri)            (None, 99681, 48)    0           cropping2d[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 99681, 5)     245         upper_tri[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "switch_reverse_triu (SwitchReve (None, 99681, 5)     0           dense[0][0]                      \n",
      "                                                                 stochastic_reverse_complement[0][\n",
      "==================================================================================================\n",
      "Total params: 751,653\n",
      "Trainable params: 746,149\n",
      "Non-trainable params: 5,504\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "human_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.convolutional.Conv1D at 0x1a3a001780>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only relevant if model was compile 1 time\n",
    "human_model.get_layer(\"conv1d_2\") # layer from which we start copying weights\n",
    "human_model.get_layer(\"batch_normalization_40\") # layer at which we stop copying weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_model_layers = []\n",
    "for layer in human_model.layers[12:-7]:\n",
    "    if 'conv1d' in layer.name or 'batch_normalization' in layer.name or 'conv2d' in layer.name:\n",
    "        human_model_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 524288\n",
    "sequence = tf.keras.Input(shape=(SEQ_LENGTH, 4), name='sequence')\n",
    "\n",
    "current = sequence\n",
    "\n",
    "# First 1D convolution\n",
    "current = conv_block(current, filters=96, kernel_size=11, pool_size=2, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "# Multiple (11) 1D convolutions in a tower to arrive to 1024bp representation in 1D vectors\n",
    "current = conv_tower(current, filters_init=96, filters_mult=1.0, kernel_size=5, pool_size=2, repeat=9, \n",
    "                     batch_norm=True, bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# Dilated residual layers\n",
    "current = dilated_residual(current, filters=48, rate_mult=1.75, repeat=8, dropout=0.4, batch_norm=True, \n",
    "                           bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# Bottleneck 1D convolution\n",
    "current = conv_block(current, filters=64, kernel_size=5, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "# final activation\n",
    "current = activate(current, \"relu\")\n",
    "\n",
    "\n",
    "# HEAD:\n",
    "current = one_to_two(current)\n",
    "current = concat_dist_2d(current)\n",
    "current = conv_block_2d(current, filters=48, kernel_size=3, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "current = symmetrize_2d(current)\n",
    "current = dilated_residual_2d(current, filters=24, kernel_size=3, rate_mult=1.75, repeat=6, dropout=0.1,\n",
    "                              batch_norm=True, bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# TODO: TRY WITHOUT CROP\n",
    "# current = cropping_2d(current, cropping=26)\n",
    "\n",
    "current = upper_tri(current, diagonal_offset=2)\n",
    "\n",
    "current = dense(current, units=1, activation=\"linear\")\n",
    "# current = dense(current, units=5, activation=\"linear\")\n",
    "\n",
    "# current = SwitchReverse()([current, reverse_bool])\n",
    "\n",
    "# make model\n",
    "drosophila_model = tf.keras.Model(inputs=sequence, outputs=current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           [(None, 524288, 4)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_289 (ReLU)                (None, 524288, 4)    0           sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_191 (Conv1D)             (None, 524288, 96)   4224        re_lu_289[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_282 (BatchN (None, 524288, 96)   384         conv1d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_72 (MaxPooling1D) (None, 262144, 96)   0           batch_normalization_282[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_290 (ReLU)                (None, 262144, 96)   0           max_pooling1d_72[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_192 (Conv1D)             (None, 262144, 96)   46080       re_lu_290[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_283 (BatchN (None, 262144, 96)   384         conv1d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_73 (MaxPooling1D) (None, 131072, 96)   0           batch_normalization_283[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_291 (ReLU)                (None, 131072, 96)   0           max_pooling1d_73[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_193 (Conv1D)             (None, 131072, 96)   46080       re_lu_291[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_284 (BatchN (None, 131072, 96)   384         conv1d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_74 (MaxPooling1D) (None, 65536, 96)    0           batch_normalization_284[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_292 (ReLU)                (None, 65536, 96)    0           max_pooling1d_74[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_194 (Conv1D)             (None, 65536, 96)    46080       re_lu_292[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_285 (BatchN (None, 65536, 96)    384         conv1d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_75 (MaxPooling1D) (None, 32768, 96)    0           batch_normalization_285[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_293 (ReLU)                (None, 32768, 96)    0           max_pooling1d_75[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_195 (Conv1D)             (None, 32768, 96)    46080       re_lu_293[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_286 (BatchN (None, 32768, 96)    384         conv1d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_76 (MaxPooling1D) (None, 16384, 96)    0           batch_normalization_286[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_294 (ReLU)                (None, 16384, 96)    0           max_pooling1d_76[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_196 (Conv1D)             (None, 16384, 96)    46080       re_lu_294[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_287 (BatchN (None, 16384, 96)    384         conv1d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_77 (MaxPooling1D) (None, 8192, 96)     0           batch_normalization_287[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_295 (ReLU)                (None, 8192, 96)     0           max_pooling1d_77[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_197 (Conv1D)             (None, 8192, 96)     46080       re_lu_295[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_288 (BatchN (None, 8192, 96)     384         conv1d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_78 (MaxPooling1D) (None, 4096, 96)     0           batch_normalization_288[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_296 (ReLU)                (None, 4096, 96)     0           max_pooling1d_78[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_198 (Conv1D)             (None, 4096, 96)     46080       re_lu_296[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_289 (BatchN (None, 4096, 96)     384         conv1d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_79 (MaxPooling1D) (None, 2048, 96)     0           batch_normalization_289[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_297 (ReLU)                (None, 2048, 96)     0           max_pooling1d_79[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_199 (Conv1D)             (None, 2048, 96)     46080       re_lu_297[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_290 (BatchN (None, 2048, 96)     384         conv1d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_80 (MaxPooling1D) (None, 1024, 96)     0           batch_normalization_290[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_298 (ReLU)                (None, 1024, 96)     0           max_pooling1d_80[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_200 (Conv1D)             (None, 1024, 96)     46080       re_lu_298[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_291 (BatchN (None, 1024, 96)     384         conv1d_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_81 (MaxPooling1D) (None, 512, 96)      0           batch_normalization_291[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_299 (ReLU)                (None, 512, 96)      0           max_pooling1d_81[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_201 (Conv1D)             (None, 512, 48)      13824       re_lu_299[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_292 (BatchN (None, 512, 48)      192         conv1d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_300 (ReLU)                (None, 512, 48)      0           batch_normalization_292[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_202 (Conv1D)             (None, 512, 96)      4608        re_lu_300[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_293 (BatchN (None, 512, 96)      384         conv1d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)            (None, 512, 96)      0           batch_normalization_293[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_98 (Add)                    (None, 512, 96)      0           max_pooling1d_81[0][0]           \n",
      "                                                                 dropout_98[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_301 (ReLU)                (None, 512, 96)      0           add_98[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_203 (Conv1D)             (None, None, 48)     13824       re_lu_301[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_294 (BatchN (None, None, 48)     192         conv1d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_302 (ReLU)                (None, None, 48)     0           batch_normalization_294[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_204 (Conv1D)             (None, None, 96)     4608        re_lu_302[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_295 (BatchN (None, None, 96)     384         conv1d_204[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_99 (Dropout)            (None, None, 96)     0           batch_normalization_295[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_99 (Add)                    (None, 512, 96)      0           add_98[0][0]                     \n",
      "                                                                 dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_303 (ReLU)                (None, 512, 96)      0           add_99[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_205 (Conv1D)             (None, None, 48)     13824       re_lu_303[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_296 (BatchN (None, None, 48)     192         conv1d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_304 (ReLU)                (None, None, 48)     0           batch_normalization_296[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_206 (Conv1D)             (None, None, 96)     4608        re_lu_304[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_297 (BatchN (None, None, 96)     384         conv1d_206[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_100 (Dropout)           (None, None, 96)     0           batch_normalization_297[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_100 (Add)                   (None, 512, 96)      0           add_99[0][0]                     \n",
      "                                                                 dropout_100[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_305 (ReLU)                (None, 512, 96)      0           add_100[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_207 (Conv1D)             (None, None, 48)     13824       re_lu_305[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_298 (BatchN (None, None, 48)     192         conv1d_207[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_306 (ReLU)                (None, None, 48)     0           batch_normalization_298[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_208 (Conv1D)             (None, None, 96)     4608        re_lu_306[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_299 (BatchN (None, None, 96)     384         conv1d_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_101 (Dropout)           (None, None, 96)     0           batch_normalization_299[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_101 (Add)                   (None, 512, 96)      0           add_100[0][0]                    \n",
      "                                                                 dropout_101[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_307 (ReLU)                (None, 512, 96)      0           add_101[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_209 (Conv1D)             (None, None, 48)     13824       re_lu_307[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_300 (BatchN (None, None, 48)     192         conv1d_209[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_308 (ReLU)                (None, None, 48)     0           batch_normalization_300[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_210 (Conv1D)             (None, None, 96)     4608        re_lu_308[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_301 (BatchN (None, None, 96)     384         conv1d_210[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_102 (Dropout)           (None, None, 96)     0           batch_normalization_301[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_102 (Add)                   (None, 512, 96)      0           add_101[0][0]                    \n",
      "                                                                 dropout_102[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_309 (ReLU)                (None, 512, 96)      0           add_102[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_211 (Conv1D)             (None, None, 48)     13824       re_lu_309[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_302 (BatchN (None, None, 48)     192         conv1d_211[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_310 (ReLU)                (None, None, 48)     0           batch_normalization_302[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_212 (Conv1D)             (None, None, 96)     4608        re_lu_310[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_303 (BatchN (None, None, 96)     384         conv1d_212[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_103 (Dropout)           (None, None, 96)     0           batch_normalization_303[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_103 (Add)                   (None, 512, 96)      0           add_102[0][0]                    \n",
      "                                                                 dropout_103[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_311 (ReLU)                (None, 512, 96)      0           add_103[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_213 (Conv1D)             (None, None, 48)     13824       re_lu_311[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_304 (BatchN (None, None, 48)     192         conv1d_213[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_312 (ReLU)                (None, None, 48)     0           batch_normalization_304[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_214 (Conv1D)             (None, None, 96)     4608        re_lu_312[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_305 (BatchN (None, None, 96)     384         conv1d_214[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_104 (Dropout)           (None, None, 96)     0           batch_normalization_305[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_104 (Add)                   (None, 512, 96)      0           add_103[0][0]                    \n",
      "                                                                 dropout_104[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_313 (ReLU)                (None, 512, 96)      0           add_104[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_215 (Conv1D)             (None, None, 48)     13824       re_lu_313[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_306 (BatchN (None, None, 48)     192         conv1d_215[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_314 (ReLU)                (None, None, 48)     0           batch_normalization_306[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_216 (Conv1D)             (None, None, 96)     4608        re_lu_314[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_307 (BatchN (None, None, 96)     384         conv1d_216[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_105 (Dropout)           (None, None, 96)     0           batch_normalization_307[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_105 (Add)                   (None, 512, 96)      0           add_104[0][0]                    \n",
      "                                                                 dropout_105[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_315 (ReLU)                (None, 512, 96)      0           add_105[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_217 (Conv1D)             (None, 512, 64)      30720       re_lu_315[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_308 (BatchN (None, 512, 64)      256         conv1d_217[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_316 (ReLU)                (None, 512, 64)      0           batch_normalization_308[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "one_to_two_6 (OneToTwo)         (None, 512, 512, 64) 0           re_lu_316[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concat_dist2d_7 (ConcatDist2D)  (None, 512, 512, 65) 0           one_to_two_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_317 (ReLU)                (None, 512, 512, 65) 0           concat_dist2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 512, 512, 48) 28080       re_lu_317[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_309 (BatchN (None, 512, 512, 48) 192         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_49 (Symmetrize2D)  (None, 512, 512, 48) 0           batch_normalization_309[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_318 (ReLU)                (None, 512, 512, 48) 0           symmetrize2d_49[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 512, 512, 24) 10368       re_lu_318[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_310 (BatchN (None, 512, 512, 24) 96          conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_319 (ReLU)                (None, 512, 512, 24) 0           batch_normalization_310[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 512, 512, 48) 1152        re_lu_319[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_311 (BatchN (None, 512, 512, 48) 192         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_106 (Dropout)           (None, 512, 512, 48) 0           batch_normalization_311[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_106 (Add)                   (None, 512, 512, 48) 0           symmetrize2d_49[0][0]            \n",
      "                                                                 dropout_106[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_50 (Symmetrize2D)  (None, 512, 512, 48) 0           add_106[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_320 (ReLU)                (None, 512, 512, 48) 0           symmetrize2d_50[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, None, None, 2 10368       re_lu_320[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_312 (BatchN (None, None, None, 2 96          conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_321 (ReLU)                (None, None, None, 2 0           batch_normalization_312[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, None, None, 4 1152        re_lu_321[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_313 (BatchN (None, None, None, 4 192         conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_107 (Dropout)           (None, None, None, 4 0           batch_normalization_313[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_107 (Add)                   (None, 512, 512, 48) 0           symmetrize2d_50[0][0]            \n",
      "                                                                 dropout_107[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_51 (Symmetrize2D)  (None, 512, 512, 48) 0           add_107[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_322 (ReLU)                (None, 512, 512, 48) 0           symmetrize2d_51[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, None, None, 2 10368       re_lu_322[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_314 (BatchN (None, None, None, 2 96          conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_323 (ReLU)                (None, None, None, 2 0           batch_normalization_314[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, None, None, 4 1152        re_lu_323[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_315 (BatchN (None, None, None, 4 192         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_108 (Dropout)           (None, None, None, 4 0           batch_normalization_315[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_108 (Add)                   (None, 512, 512, 48) 0           symmetrize2d_51[0][0]            \n",
      "                                                                 dropout_108[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_52 (Symmetrize2D)  (None, 512, 512, 48) 0           add_108[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_324 (ReLU)                (None, 512, 512, 48) 0           symmetrize2d_52[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, None, None, 2 10368       re_lu_324[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_316 (BatchN (None, None, None, 2 96          conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_325 (ReLU)                (None, None, None, 2 0           batch_normalization_316[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, None, None, 4 1152        re_lu_325[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_317 (BatchN (None, None, None, 4 192         conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_109 (Dropout)           (None, None, None, 4 0           batch_normalization_317[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_109 (Add)                   (None, 512, 512, 48) 0           symmetrize2d_52[0][0]            \n",
      "                                                                 dropout_109[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_53 (Symmetrize2D)  (None, 512, 512, 48) 0           add_109[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_326 (ReLU)                (None, 512, 512, 48) 0           symmetrize2d_53[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, None, None, 2 10368       re_lu_326[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_318 (BatchN (None, None, None, 2 96          conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_327 (ReLU)                (None, None, None, 2 0           batch_normalization_318[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, None, None, 4 1152        re_lu_327[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_319 (BatchN (None, None, None, 4 192         conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_110 (Dropout)           (None, None, None, 4 0           batch_normalization_319[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_110 (Add)                   (None, 512, 512, 48) 0           symmetrize2d_53[0][0]            \n",
      "                                                                 dropout_110[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_54 (Symmetrize2D)  (None, 512, 512, 48) 0           add_110[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_328 (ReLU)                (None, 512, 512, 48) 0           symmetrize2d_54[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, None, None, 2 10368       re_lu_328[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_320 (BatchN (None, None, None, 2 96          conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_329 (ReLU)                (None, None, None, 2 0           batch_normalization_320[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, None, None, 4 1152        re_lu_329[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_321 (BatchN (None, None, None, 4 192         conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_111 (Dropout)           (None, None, None, 4 0           batch_normalization_321[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_111 (Add)                   (None, 512, 512, 48) 0           symmetrize2d_54[0][0]            \n",
      "                                                                 dropout_111[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "symmetrize2d_55 (Symmetrize2D)  (None, 512, 512, 48) 0           add_111[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "upper_tri_7 (UpperTri)          (None, 130305, 48)   0           symmetrize2d_55[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 130305, 1)    49          upper_tri_7[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 704,993\n",
      "Trainable params: 699,681\n",
      "Non-trainable params: 5,312\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "drosophila_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weights from the fudenberg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for layer in drosophila_model.layers[6:]:\n",
    "    if 'conv1d' in layer.name or 'batch_normalization' in layer.name or 'conv2d' in layer.name:\n",
    "        layer.set_weights(human_model_layers[count].get_weights())\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Осталось натренировать с новым тренировочным множеством"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 524288\n",
    "sequence = tf.keras.Input(shape=(SEQ_LENGTH, 4), name='sequence')\n",
    "\n",
    "current = sequence\n",
    "\n",
    "# Augmentation - Enable it later (after good performance on the training set)\n",
    "# current, reverse_bool = StochasticReverseComplement()(current)\n",
    "# augment_shift = 11\n",
    "# current = StochasticShift(augment_shift)(current)\n",
    "\n",
    "# TRUNK:\n",
    "\n",
    "\n",
    "# First 1D convolution\n",
    "current = conv_block(current, filters=96, kernel_size=11, pool_size=2, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "\n",
    "# Change for repeat = 9\n",
    "\n",
    "# Multiple (11) 1D convolutions in a tower to arrive to 2048bp representation in 1D vectors\n",
    "current = conv_tower(current, filters_init=96, filters_mult=1.0, kernel_size=5, pool_size=2, repeat=9, \n",
    "                     batch_norm=True, bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# Dilated residual layers\n",
    "current = dilated_residual(current, filters=48, rate_mult=1.75, repeat=8, dropout=0.4, batch_norm=True, \n",
    "                           bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# Bottleneck 1D convolution\n",
    "current = conv_block(current, filters=64, kernel_size=5, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "# final activation\n",
    "current = activate(current, \"relu\")\n",
    "\n",
    "\n",
    "# HEAD:\n",
    "current = one_to_two(current)\n",
    "current = concat_dist_2d(current)\n",
    "current = conv_block_2d(current, filters=48, kernel_size=3, batch_norm=True, bn_momentum=0.9265,\n",
    "                     activation=\"relu\")\n",
    "\n",
    "current = symmetrize_2d(current)\n",
    "current = dilated_residual_2d(current, filters=24, kernel_size=3, rate_mult=1.75, repeat=6, dropout=0.1,\n",
    "                              batch_norm=True, bn_momentum=0.9265, activation=\"relu\")\n",
    "\n",
    "# TODO: TRY WITHOUT CROP\n",
    "current = cropping_2d(current, cropping=26)\n",
    "\n",
    "current = upper_tri(current, diagonal_offset=2)\n",
    "\n",
    "#current = dense(current, units=1, activation=\"linear\")\n",
    "current = dense(current, units=5, activation=\"linear\")\n",
    "\n",
    "# current = SwitchReverse()([current, reverse_bool])\n",
    "\n",
    "# make model\n",
    "model = tf.keras.Model(inputs=sequence, outputs=current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Transfer learning method\n",
    "################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
